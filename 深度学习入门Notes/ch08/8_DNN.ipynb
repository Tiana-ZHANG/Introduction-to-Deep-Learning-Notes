{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 深度学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch08/deep_convnet.py\n",
    "# 使用训练好的权重参数\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"识别率为99%以上的高精度的ConvNet\n",
    "\n",
    "    网络结构如下所示\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 初始化权重===========\n",
    "        # 各层的神经元平均与前一层的几个神经元有连接（TODO:自动计算）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # 使用ReLU的情况下推荐的初始值\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 生成层===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.334129762457887\n",
      "=== epoch:1, train acc:0.105, test acc:0.112 ===\n",
      "train loss:2.273767799081687\n",
      "train loss:2.2706098087969986\n",
      "train loss:2.279419756896308\n",
      "train loss:2.2761699693337176\n",
      "train loss:2.2712994064200838\n",
      "train loss:2.2439188944899997\n",
      "train loss:2.2419777141732493\n",
      "train loss:2.243157721334387\n",
      "train loss:2.2674756385566432\n",
      "train loss:2.245853319271882\n",
      "train loss:2.1737735775466076\n",
      "train loss:2.21923473921106\n",
      "train loss:2.222798533442389\n",
      "train loss:2.1743621011485583\n",
      "train loss:2.1155902189541873\n",
      "train loss:2.1890429874607467\n",
      "train loss:2.1718879542414147\n",
      "train loss:2.150426427321084\n",
      "train loss:2.1228118337610895\n",
      "train loss:2.157170140554426\n",
      "train loss:2.025333653922487\n",
      "train loss:2.1140285303121913\n",
      "train loss:2.1069657079289303\n",
      "train loss:1.8692167824437163\n",
      "train loss:2.0023417644499206\n",
      "train loss:2.124496139463732\n",
      "train loss:2.0548622457867323\n",
      "train loss:1.8989265225435872\n",
      "train loss:1.8382093213513215\n",
      "train loss:1.8435816431480152\n",
      "train loss:1.8798344050349423\n",
      "train loss:1.9023011962386527\n",
      "train loss:1.942956024714727\n",
      "train loss:2.010655956183599\n",
      "train loss:1.9370739831194919\n",
      "train loss:1.7418355486059727\n",
      "train loss:2.013795272984256\n",
      "train loss:1.8692341426569061\n",
      "train loss:1.7555218011130451\n",
      "train loss:1.906277227120901\n",
      "train loss:1.8498340016287182\n",
      "train loss:1.9509515733391618\n",
      "train loss:1.778543867260602\n",
      "train loss:1.8005840361148366\n",
      "train loss:1.8549828257038712\n",
      "train loss:1.873405587245285\n",
      "train loss:2.119737354787934\n",
      "train loss:1.8349058885997382\n",
      "train loss:1.6624053569376216\n",
      "train loss:1.6988189960307005\n",
      "train loss:1.8218763571894911\n",
      "train loss:1.844490250073742\n",
      "train loss:1.8361441029676644\n",
      "train loss:1.8130533702273701\n",
      "train loss:1.8694009778625347\n",
      "train loss:1.778159646117817\n",
      "train loss:1.6882617212340494\n",
      "train loss:1.7373308573641417\n",
      "train loss:1.9440167099110266\n",
      "train loss:1.749820052864084\n",
      "train loss:1.8034341016912503\n",
      "train loss:1.8070748184115382\n",
      "train loss:1.8264898038700559\n",
      "train loss:1.6715495778882214\n",
      "train loss:1.7019350506078623\n",
      "train loss:1.562777576480408\n",
      "train loss:1.6816428026413355\n",
      "train loss:1.7053747836280033\n",
      "train loss:1.6434185139354447\n",
      "train loss:1.9365532880425702\n",
      "train loss:1.7197964612828764\n",
      "train loss:1.6401021510744125\n",
      "train loss:1.5799792970229307\n",
      "train loss:1.7870116526136344\n",
      "train loss:1.6743221767400562\n",
      "train loss:1.5030691894964254\n",
      "train loss:1.6077159880464171\n",
      "train loss:1.6784517915455508\n",
      "train loss:1.6452508293211199\n",
      "train loss:1.5822518602569753\n",
      "train loss:1.579648817246578\n",
      "train loss:1.4429886503575724\n",
      "train loss:1.5987235984893013\n",
      "train loss:1.4467168918960147\n",
      "train loss:1.480497495044331\n",
      "train loss:1.621829901348907\n",
      "train loss:1.5981885992067064\n",
      "train loss:1.440090356983577\n",
      "train loss:1.505420655365626\n",
      "train loss:1.664129616410056\n",
      "train loss:1.4511844590146752\n",
      "train loss:1.722253489007571\n",
      "train loss:1.5266651594744318\n",
      "train loss:1.6382210781035524\n",
      "train loss:1.6115744764749218\n",
      "train loss:1.6562163802923686\n",
      "train loss:1.5179344371842196\n",
      "train loss:1.8074744715489675\n",
      "train loss:1.4587449334353315\n",
      "train loss:1.6227717310547871\n",
      "train loss:1.554339189604576\n",
      "train loss:1.6359652373068228\n",
      "train loss:1.5000191473637075\n",
      "train loss:1.5158096549550109\n",
      "train loss:1.4953095016428466\n",
      "train loss:1.4329743629778553\n",
      "train loss:1.408146068522089\n",
      "train loss:1.5820541780828605\n",
      "train loss:1.64799161157053\n",
      "train loss:1.6302237351554851\n",
      "train loss:1.680700299849149\n",
      "train loss:1.4989647404404247\n",
      "train loss:1.437375532582233\n",
      "train loss:1.6108637608217753\n",
      "train loss:1.6397615566741472\n",
      "train loss:1.6513427476134601\n",
      "train loss:1.408498830194273\n",
      "train loss:1.353010377741696\n",
      "train loss:1.4230838538604476\n",
      "train loss:1.4168248307021218\n",
      "train loss:1.499031730211898\n",
      "train loss:1.6372540525917885\n",
      "train loss:1.2792208818514756\n",
      "train loss:1.572765078889692\n",
      "train loss:1.289594773922301\n",
      "train loss:1.2870115259558887\n",
      "train loss:1.558368398694723\n",
      "train loss:1.3302112047066037\n",
      "train loss:1.5810338734562996\n",
      "train loss:1.6320184402148183\n",
      "train loss:1.6015459470887101\n",
      "train loss:1.4705599434867014\n",
      "train loss:1.379230930013779\n",
      "train loss:1.3960236135738762\n",
      "train loss:1.5274517346007723\n",
      "train loss:1.6030512027303454\n",
      "train loss:1.52473885437466\n",
      "train loss:1.3297906223902354\n",
      "train loss:1.4210184592452086\n",
      "train loss:1.3822388798478693\n",
      "train loss:1.5910376112572118\n",
      "train loss:1.3745456532009017\n",
      "train loss:1.4835456407329162\n",
      "train loss:1.4431179663939162\n",
      "train loss:1.4529431825464763\n",
      "train loss:1.1776505385842173\n",
      "train loss:1.3653062985182591\n",
      "train loss:1.2277863071580895\n",
      "train loss:1.4009393843602234\n",
      "train loss:1.3615099013636047\n",
      "train loss:1.2503429058877602\n",
      "train loss:1.4901725894944249\n",
      "train loss:1.2816205750420273\n",
      "train loss:1.4592643334862943\n",
      "train loss:1.5222733500348422\n",
      "train loss:1.4610509681277577\n",
      "train loss:1.2822123405883181\n",
      "train loss:1.3968679780419482\n",
      "train loss:1.2971181815613522\n",
      "train loss:1.4576428935513985\n",
      "train loss:1.2564800779712937\n",
      "train loss:1.2325011177960077\n",
      "train loss:1.4994305199446791\n",
      "train loss:1.2871892144398913\n",
      "train loss:1.3188474659949865\n",
      "train loss:1.592809242374692\n",
      "train loss:1.5888423607669828\n",
      "train loss:1.418668996477865\n",
      "train loss:1.412029495033891\n",
      "train loss:1.3812835016757719\n",
      "train loss:1.3607209977919852\n",
      "train loss:1.2444114184335908\n",
      "train loss:1.2878949400318251\n",
      "train loss:1.4635949677284648\n",
      "train loss:1.5080428008045765\n",
      "train loss:1.4645915527606237\n",
      "train loss:1.5263317791598228\n",
      "train loss:1.334586214622813\n",
      "train loss:1.2554963936493173\n",
      "train loss:1.2190802007289767\n",
      "train loss:1.3777313636259754\n",
      "train loss:1.408177868120016\n",
      "train loss:1.4297729603845568\n",
      "train loss:1.2906858751654822\n",
      "train loss:1.169665396491725\n",
      "train loss:1.3259188533082387\n",
      "train loss:1.3131861140249539\n",
      "train loss:1.3595987177039655\n",
      "train loss:1.1850522191433135\n",
      "train loss:1.217761646479811\n",
      "train loss:1.5953030000648787\n",
      "train loss:1.3273290264652902\n",
      "train loss:1.2454006750735451\n",
      "train loss:1.1261707054392922\n",
      "train loss:1.3064718636331403\n",
      "train loss:1.3094080266512187\n",
      "train loss:1.3079378248916222\n",
      "train loss:1.258710789340609\n",
      "train loss:1.5561868274958517\n",
      "train loss:1.3747525360429955\n",
      "train loss:1.186465857244773\n",
      "train loss:1.1819886564266637\n",
      "train loss:1.30515357144512\n",
      "train loss:1.2336254533989577\n",
      "train loss:1.3268935032675133\n",
      "train loss:1.2195400215067702\n",
      "train loss:1.3456388947995277\n",
      "train loss:1.379291139506952\n",
      "train loss:1.2002124268791916\n",
      "train loss:1.3638000351037995\n",
      "train loss:1.38919887428957\n",
      "train loss:1.169273888023325\n",
      "train loss:1.1735873494644047\n",
      "train loss:1.4343300676020063\n",
      "train loss:1.3823192687983927\n",
      "train loss:1.2699826676700474\n",
      "train loss:1.276017338522245\n",
      "train loss:1.3577871169069056\n",
      "train loss:1.2257437277931769\n",
      "train loss:1.5266156696432063\n",
      "train loss:1.4107149140397706\n",
      "train loss:1.340886317615212\n",
      "train loss:1.250170690585182\n",
      "train loss:1.2523142966769414\n",
      "train loss:1.3532755750706795\n",
      "train loss:1.1673882628865384\n",
      "train loss:1.2110207946147205\n",
      "train loss:1.4849566699523438\n",
      "train loss:1.4106105558798736\n",
      "train loss:1.4101020203326415\n",
      "train loss:1.234811410732682\n",
      "train loss:1.1806306998594547\n",
      "train loss:1.2259933282332875\n",
      "train loss:1.0332816541722267\n",
      "train loss:1.2931403553218985\n",
      "train loss:1.2407338210939403\n",
      "train loss:1.3073382861540497\n",
      "train loss:1.2870005708363335\n",
      "train loss:1.153979963960647\n",
      "train loss:1.121087042459689\n",
      "train loss:1.2123626837898334\n",
      "train loss:0.8682749995315012\n",
      "train loss:1.0025135164302363\n",
      "train loss:1.2334711144795303\n",
      "train loss:1.165085060541635\n",
      "train loss:1.150005049739949\n",
      "train loss:1.37538544584593\n",
      "train loss:1.038668600068856\n",
      "train loss:0.937118541144927\n",
      "train loss:1.2318718016018466\n",
      "train loss:1.2579221560310372\n",
      "train loss:1.3646259120221216\n",
      "train loss:1.1148173927856753\n",
      "train loss:1.2043618888291554\n",
      "train loss:1.4050035572023916\n",
      "train loss:1.2080623939680504\n",
      "train loss:1.3185034427847555\n",
      "train loss:0.9905987981985517\n",
      "train loss:1.3003016715959044\n",
      "train loss:1.297342035562919\n",
      "train loss:1.2118003067921708\n",
      "train loss:1.1622495050681214\n",
      "train loss:1.1788167009156962\n",
      "train loss:1.2506940078617648\n",
      "train loss:1.1361753322736459\n",
      "train loss:1.0229030904474654\n",
      "train loss:1.1950017650541596\n",
      "train loss:1.3283608349539215\n",
      "train loss:1.1530906852158904\n",
      "train loss:1.0386557972911246\n",
      "train loss:1.2646766235896478\n",
      "train loss:1.2113387222984258\n",
      "train loss:1.1033421545738287\n",
      "train loss:1.1391490408378235\n",
      "train loss:1.183814394330418\n",
      "train loss:1.3807530234440102\n",
      "train loss:1.193257234148671\n",
      "train loss:1.036615774529365\n",
      "train loss:1.013980226592282\n",
      "train loss:1.1350090769307577\n",
      "train loss:1.2483710944608217\n",
      "train loss:1.0095023075584735\n",
      "train loss:1.0603514417721553\n",
      "train loss:1.3001868855842382\n",
      "train loss:1.4012247585701294\n",
      "train loss:1.2861117809262286\n",
      "train loss:1.4446977120947857\n",
      "train loss:1.1111264806759096\n",
      "train loss:1.2924980431163249\n",
      "train loss:1.0345371320518955\n",
      "train loss:1.2016593721441677\n",
      "train loss:1.0704719325942949\n",
      "train loss:1.1229540650360892\n",
      "train loss:1.0555995644299905\n",
      "train loss:0.9911907849666386\n",
      "train loss:1.071749828727277\n",
      "train loss:1.1942442540732683\n",
      "train loss:1.0849746172291361\n",
      "train loss:1.0350952231216424\n",
      "train loss:1.123080242274139\n",
      "train loss:1.1974883282166822\n",
      "train loss:1.044237887352446\n",
      "train loss:1.1900938823486213\n",
      "train loss:1.1456442973220107\n",
      "train loss:1.1263370318377475\n",
      "train loss:1.1761883001563398\n",
      "train loss:1.0331246863252193\n",
      "train loss:1.0148900955617497\n",
      "train loss:1.30007907417634\n",
      "train loss:1.165641738920965\n",
      "train loss:1.280593029887465\n",
      "train loss:1.2450605637532532\n",
      "train loss:1.0721922811273021\n",
      "train loss:1.0135382717195052\n",
      "train loss:1.3254497523643045\n",
      "train loss:1.2357440033073004\n",
      "train loss:1.023782018468249\n",
      "train loss:1.1743335401203299\n",
      "train loss:1.0110669856732266\n",
      "train loss:1.1288109982605865\n",
      "train loss:0.9598291832948324\n",
      "train loss:1.1588501910169087\n",
      "train loss:1.1438903652352765\n",
      "train loss:1.2072052764468155\n",
      "train loss:1.0991631470738865\n",
      "train loss:1.044363481602016\n",
      "train loss:1.1047170985841595\n",
      "train loss:1.248791472922596\n",
      "train loss:1.0519441696496685\n",
      "train loss:1.0870275463504242\n",
      "train loss:1.062433769852062\n",
      "train loss:1.0382782788744036\n",
      "train loss:1.0918851453547038\n",
      "train loss:0.9972597136696897\n",
      "train loss:1.2662663135546617\n",
      "train loss:0.9517947581228812\n",
      "train loss:1.0786409394035432\n",
      "train loss:1.0490183649151914\n",
      "train loss:1.1724860046445664\n",
      "train loss:1.128170635823829\n",
      "train loss:1.0657029448719342\n",
      "train loss:1.17541222678536\n",
      "train loss:1.1877103509805849\n",
      "train loss:1.2805910434868975\n",
      "train loss:1.2032231595218645\n",
      "train loss:1.222646173721535\n",
      "train loss:0.8799734627039625\n",
      "train loss:1.1445955510713142\n",
      "train loss:1.300184496525978\n",
      "train loss:1.1157336653947465\n",
      "train loss:1.061185153171224\n",
      "train loss:1.1171962366883579\n",
      "train loss:0.9151944743416215\n",
      "train loss:1.1661793658936788\n",
      "train loss:1.0254486743677471\n",
      "train loss:1.1274505690154897\n",
      "train loss:1.1520810769396492\n",
      "train loss:1.1344424461995148\n",
      "train loss:0.9318129142577928\n",
      "train loss:1.1971331298284504\n",
      "train loss:1.1350743719658944\n",
      "train loss:1.2076810285951491\n",
      "train loss:1.0374278596719941\n",
      "train loss:1.209576648511578\n",
      "train loss:1.091947936470217\n",
      "train loss:1.2056432093862284\n",
      "train loss:1.1079610187614144\n",
      "train loss:1.3904932805414945\n",
      "train loss:1.266410845411033\n",
      "train loss:1.0005225239700257\n",
      "train loss:1.0210859543565634\n",
      "train loss:0.802456311912299\n",
      "train loss:0.98140170466883\n",
      "train loss:1.0616446290601869\n",
      "train loss:1.0920674993132646\n",
      "train loss:1.1903903928215842\n",
      "train loss:1.0610697689508446\n",
      "train loss:1.0711912660539467\n",
      "train loss:1.0917230277416126\n",
      "train loss:1.1133273453679151\n",
      "train loss:1.0625634526888057\n",
      "train loss:1.1225467002244065\n",
      "train loss:1.1501720062320775\n",
      "train loss:1.1797295182087826\n",
      "train loss:1.009965915875184\n",
      "train loss:1.0718539406404637\n",
      "train loss:0.9797531168169126\n",
      "train loss:1.1803222444292834\n",
      "train loss:1.0171916045852452\n",
      "train loss:1.1048458643474646\n",
      "train loss:1.1330685533914273\n",
      "train loss:1.0069744807176435\n",
      "train loss:1.10180295016441\n",
      "train loss:1.0027628056866396\n",
      "train loss:0.8927641286058755\n",
      "train loss:0.8535613544343947\n",
      "train loss:1.0943647694020533\n",
      "train loss:1.223626859909502\n",
      "train loss:1.1051570433968634\n",
      "train loss:1.1611666700576904\n",
      "train loss:1.033612985088419\n",
      "train loss:1.0535234684808026\n",
      "train loss:1.0549885057325699\n",
      "train loss:1.1961405246696413\n",
      "train loss:1.108074407382677\n",
      "train loss:0.9472997807823259\n",
      "train loss:1.079749600450511\n",
      "train loss:1.1527767353632148\n",
      "train loss:0.8347553882680966\n",
      "train loss:1.0894056267794385\n",
      "train loss:1.0811872501035804\n",
      "train loss:1.0196103631209616\n",
      "train loss:0.9952848421966533\n",
      "train loss:1.087737505754775\n",
      "train loss:1.0984634032561635\n",
      "train loss:0.9264808361810448\n",
      "train loss:0.8160916878845557\n",
      "train loss:1.1380364572528905\n",
      "train loss:1.018191605798222\n",
      "train loss:0.9492574850697254\n",
      "train loss:1.054233215148341\n",
      "train loss:0.9540000132804062\n",
      "train loss:0.9818125266894603\n",
      "train loss:0.9735663359787661\n",
      "train loss:1.1043489859431626\n",
      "train loss:1.0504512331968812\n",
      "train loss:1.0727425797049552\n",
      "train loss:1.0873690187464988\n",
      "train loss:1.0323083746552215\n",
      "train loss:1.0810068252254679\n",
      "train loss:1.166863114293685\n",
      "train loss:1.2382825687658114\n",
      "train loss:1.0334681602128852\n",
      "train loss:0.9882192102778926\n",
      "train loss:1.0699754187259178\n",
      "train loss:1.1697010798213903\n",
      "train loss:1.1408118178754012\n",
      "train loss:0.9885494040610435\n",
      "train loss:1.2145891889767642\n",
      "train loss:1.0018731368273943\n",
      "train loss:1.0557626972118572\n",
      "train loss:0.8966774088460828\n",
      "train loss:1.031005349756455\n",
      "train loss:0.9399575052068969\n",
      "train loss:1.0878584421597126\n",
      "train loss:0.9421253510880327\n",
      "train loss:1.0425891212189\n",
      "train loss:1.010901716363748\n",
      "train loss:1.1389317541075454\n",
      "train loss:1.0343736519253872\n",
      "train loss:1.07903486987264\n",
      "train loss:1.2698482030468212\n",
      "train loss:1.0101437839441485\n",
      "train loss:1.0653714337949283\n",
      "train loss:1.1309808861606525\n",
      "train loss:1.0106301863695097\n",
      "train loss:0.9566152430201962\n",
      "train loss:1.0833088810668894\n",
      "train loss:0.9895784983435132\n",
      "train loss:0.8224448826941494\n",
      "train loss:0.8663335472738136\n",
      "train loss:1.0968931212573927\n",
      "train loss:1.1930802849031028\n",
      "train loss:1.1030676292697295\n",
      "train loss:0.9919705507645021\n",
      "train loss:1.0795581046700249\n",
      "train loss:1.0485493562102624\n",
      "train loss:1.1325204566474358\n",
      "train loss:0.9594098691335077\n",
      "train loss:1.2745641006969968\n",
      "train loss:1.0700275348020685\n",
      "train loss:0.9344512668242189\n",
      "train loss:1.0627072897864995\n",
      "train loss:1.126595538662604\n",
      "train loss:1.122111361896555\n",
      "train loss:1.157946678059664\n",
      "train loss:1.0864262378958598\n",
      "train loss:1.0215284902459931\n",
      "train loss:1.0180786146023701\n",
      "train loss:1.0389747833154654\n",
      "train loss:0.9566339737290174\n",
      "train loss:0.9672647211728973\n",
      "train loss:1.1841941986627516\n",
      "train loss:0.9412267094512967\n",
      "train loss:1.0007135082331677\n",
      "train loss:0.9790035696231044\n",
      "train loss:1.0842987851219812\n",
      "train loss:1.0890735053187985\n",
      "train loss:1.118561727961146\n",
      "train loss:1.0611343416635877\n",
      "train loss:1.1384980472335706\n",
      "train loss:1.0850863021827102\n",
      "train loss:0.9786426299869175\n",
      "train loss:0.9838041906963899\n",
      "train loss:0.9715757041053797\n",
      "train loss:0.8921870098941787\n",
      "train loss:1.1180955443377052\n",
      "train loss:1.1920273500376621\n",
      "train loss:0.949336339141068\n",
      "train loss:0.7988692339942709\n",
      "train loss:0.9699012767617863\n",
      "train loss:0.9466553531166937\n",
      "train loss:1.0199330538623863\n",
      "train loss:0.92704288412059\n",
      "train loss:0.9734252116662395\n",
      "train loss:1.0157690603503815\n",
      "train loss:0.9130242768990804\n",
      "train loss:1.0584571111998435\n",
      "train loss:0.9893666608610894\n",
      "train loss:1.0412357582976828\n",
      "train loss:0.942228264967938\n",
      "train loss:0.9003353436060876\n",
      "train loss:1.0022492484634116\n",
      "train loss:0.9605900317873837\n",
      "train loss:1.1370650246437417\n",
      "train loss:0.9915706451875375\n",
      "train loss:1.1126743804912052\n",
      "train loss:1.0327583956553787\n",
      "train loss:0.9803949029569968\n",
      "train loss:1.1582903655046404\n",
      "train loss:1.0573184662854922\n",
      "train loss:1.1855279886387582\n",
      "train loss:0.9785600247143657\n",
      "train loss:1.1282220404208048\n",
      "train loss:0.9549114444800528\n",
      "train loss:1.0904762599180045\n",
      "train loss:1.1989425232124489\n",
      "train loss:1.009922341355741\n",
      "train loss:0.9583764841772604\n",
      "train loss:1.0994875586650295\n",
      "train loss:0.9210782481043444\n",
      "train loss:0.969114402404663\n",
      "train loss:0.9080506750153472\n",
      "train loss:1.2601782928710055\n",
      "train loss:1.033865790792916\n",
      "train loss:0.949694039475439\n",
      "train loss:0.980393895116641\n",
      "train loss:0.9140612201089372\n",
      "train loss:0.9877023713151155\n",
      "train loss:0.9364400827920843\n",
      "train loss:1.197137381011937\n",
      "train loss:1.0508967338035293\n",
      "train loss:1.0426209237732333\n",
      "train loss:1.1882705392487463\n",
      "train loss:0.8996614418342148\n",
      "train loss:0.9605343063087965\n",
      "train loss:1.0661357625975842\n",
      "train loss:1.2098122481763625\n",
      "train loss:1.010029593163969\n",
      "train loss:1.0269996303499354\n",
      "train loss:0.9288362701020201\n",
      "train loss:1.1847644552681513\n",
      "train loss:0.9446270722828359\n",
      "train loss:1.0450963512074787\n",
      "train loss:1.1325783625815675\n",
      "train loss:0.8652534764116993\n",
      "train loss:1.286518681569862\n",
      "train loss:0.8816869825252297\n",
      "train loss:1.056221773484625\n",
      "train loss:0.946620588695896\n",
      "train loss:1.036153077499176\n",
      "train loss:1.041042058695396\n",
      "train loss:1.100957812649131\n",
      "train loss:0.9989536638260303\n",
      "train loss:1.144448192653399\n",
      "train loss:1.026701994519514\n",
      "train loss:1.0298595635984662\n",
      "train loss:0.8235029504482252\n",
      "train loss:0.8430221035497146\n",
      "train loss:1.0489861345938076\n",
      "train loss:1.1523513785737147\n",
      "train loss:1.025737741471634\n",
      "train loss:1.1592616789570962\n",
      "train loss:1.0319282039746824\n",
      "train loss:0.9558690126251201\n",
      "train loss:1.215672819768067\n",
      "train loss:1.12144376142147\n",
      "train loss:1.0701414818619934\n",
      "train loss:1.045453131978256\n",
      "train loss:0.9868830594199768\n",
      "train loss:1.0526825000305906\n",
      "train loss:0.9892035051364008\n",
      "train loss:0.8462209990492667\n",
      "train loss:1.3599455614819258\n",
      "train loss:1.100734054879231\n",
      "train loss:1.1446616861685583\n",
      "train loss:1.055732830030823\n",
      "train loss:1.1290841768985869\n",
      "train loss:1.1840273369914622\n",
      "train loss:1.0266221020132333\n",
      "train loss:1.0167492126273074\n",
      "train loss:0.9073593622860338\n",
      "train loss:0.7964066897595814\n",
      "train loss:0.9545833391560954\n",
      "train loss:1.109920951168169\n",
      "train loss:1.0535662110486401\n",
      "train loss:1.0695870362801072\n",
      "train loss:0.9155466456769009\n",
      "train loss:0.7431158225655687\n",
      "train loss:0.9906838966162574\n",
      "=== epoch:2, train acc:0.975, test acc:0.976 ===\n",
      "train loss:1.0137949615607602\n",
      "train loss:0.9948184880728546\n",
      "train loss:0.9263527398896021\n",
      "train loss:1.1123054533328678\n",
      "train loss:1.017530963333035\n",
      "train loss:0.9346343335759607\n",
      "train loss:0.9252306932721873\n",
      "train loss:0.9371545869511961\n",
      "train loss:0.965450284186396\n",
      "train loss:1.063669067984002\n",
      "train loss:0.9002730751797408\n",
      "train loss:0.8894168246620322\n",
      "train loss:1.0847109345496444\n",
      "train loss:1.1249323303450511\n",
      "train loss:1.1048631948248233\n",
      "train loss:0.9905524815710741\n",
      "train loss:0.9213783804713305\n",
      "train loss:1.1224735942452648\n",
      "train loss:1.0689802357450824\n",
      "train loss:0.8136266948039693\n",
      "train loss:1.370494263307034\n",
      "train loss:0.9483848736036535\n",
      "train loss:1.009309829906772\n",
      "train loss:1.0277360136178424\n",
      "train loss:1.080530162196048\n",
      "train loss:1.0222258759072016\n",
      "train loss:0.8438598311672247\n",
      "train loss:0.872494125409648\n",
      "train loss:0.9495255233263714\n",
      "train loss:1.0780520406603602\n",
      "train loss:1.0159333826064252\n",
      "train loss:1.049821008265329\n",
      "train loss:0.9671754349899637\n",
      "train loss:0.920861034822288\n",
      "train loss:1.0428422037587672\n",
      "train loss:0.8376181421882289\n",
      "train loss:0.9565416369239534\n",
      "train loss:0.9766308236040513\n",
      "train loss:1.0720117216842249\n",
      "train loss:1.077498824328925\n",
      "train loss:1.1444844249117638\n",
      "train loss:0.9633836832892726\n",
      "train loss:0.9929051412553097\n",
      "train loss:1.007918218523626\n",
      "train loss:0.9139370122163196\n",
      "train loss:0.8196676815375785\n",
      "train loss:1.018037822280882\n",
      "train loss:0.9966242232945488\n",
      "train loss:1.0810041272642845\n",
      "train loss:1.1369592291321562\n",
      "train loss:0.8679765894263053\n",
      "train loss:1.0411882443537472\n",
      "train loss:1.0078899149426028\n",
      "train loss:0.8953358276890375\n",
      "train loss:1.0521466590134438\n",
      "train loss:0.9024789139906894\n",
      "train loss:1.207996627916344\n",
      "train loss:0.9544753739066634\n",
      "train loss:0.9191363181809139\n",
      "train loss:1.1598623726340866\n",
      "train loss:0.9770219894556079\n",
      "train loss:0.9750236197808089\n",
      "train loss:1.0414847045941666\n",
      "train loss:0.9646106845062237\n",
      "train loss:0.9223642266433559\n",
      "train loss:1.1057390167414212\n",
      "train loss:0.7563378385968723\n",
      "train loss:0.875195504453009\n",
      "train loss:1.0211847408445422\n",
      "train loss:0.9921533186693717\n",
      "train loss:0.7248838822816982\n",
      "train loss:1.0339497028716718\n",
      "train loss:1.0264419326628234\n",
      "train loss:0.9891464778402789\n",
      "train loss:1.0276353505427445\n",
      "train loss:0.8703958809872105\n",
      "train loss:1.0242068277157115\n",
      "train loss:0.9480936116115543\n",
      "train loss:0.9938387062173597\n",
      "train loss:1.0627153154862632\n",
      "train loss:0.917443900676012\n",
      "train loss:0.9042588247502676\n",
      "train loss:1.0723945926727119\n",
      "train loss:0.9625596974740823\n",
      "train loss:1.21321236782173\n",
      "train loss:0.9983609529373761\n",
      "train loss:1.036354082298185\n",
      "train loss:0.9206997542851723\n",
      "train loss:1.0927929018109368\n",
      "train loss:0.8974653684572322\n",
      "train loss:1.1453440634000425\n",
      "train loss:0.9266380034741529\n",
      "train loss:0.9952426944953211\n",
      "train loss:0.9869263009081664\n",
      "train loss:0.9686656114081551\n",
      "train loss:0.9704494959815196\n",
      "train loss:1.006998839828331\n",
      "train loss:0.9510171214246107\n",
      "train loss:0.9365450621009187\n",
      "train loss:0.7852115107241076\n",
      "train loss:0.8802306339141788\n",
      "train loss:0.8528791043817204\n",
      "train loss:1.1547140387003756\n",
      "train loss:1.2076915461616855\n",
      "train loss:0.8735981363204219\n",
      "train loss:0.9585610794007643\n",
      "train loss:0.8544504570969316\n",
      "train loss:1.0200748070271588\n",
      "train loss:1.0470135109636236\n",
      "train loss:1.1642127210066888\n",
      "train loss:0.9208808658295652\n",
      "train loss:0.9804407829399281\n",
      "train loss:0.9729245122743544\n",
      "train loss:1.205439682944553\n",
      "train loss:0.9638343063142991\n",
      "train loss:0.9105686787117352\n",
      "train loss:1.021988919110145\n",
      "train loss:1.0698348265400137\n",
      "train loss:0.9470543565832246\n",
      "train loss:0.9971439458035474\n",
      "train loss:0.924975454713362\n",
      "train loss:0.8474033190236319\n",
      "train loss:0.8535490212680584\n",
      "train loss:0.9121573753205897\n",
      "train loss:0.9679455078724801\n",
      "train loss:1.0626074374230101\n",
      "train loss:0.9536915642142096\n",
      "train loss:0.9297676673135886\n",
      "train loss:1.1166231979006775\n",
      "train loss:1.0624609661067352\n",
      "train loss:0.9935319658141577\n",
      "train loss:0.8741729541933152\n",
      "train loss:0.9444728838395711\n",
      "train loss:1.0540082794589232\n",
      "train loss:1.0240491801641347\n",
      "train loss:1.030707408954468\n",
      "train loss:1.079568243110573\n",
      "train loss:0.907640671130485\n",
      "train loss:0.9498597079149979\n",
      "train loss:0.9137944030694014\n",
      "train loss:0.8982399304794428\n",
      "train loss:1.1613867333916652\n",
      "train loss:0.9092067643527493\n",
      "train loss:0.8532430091365895\n",
      "train loss:0.8983872041371623\n",
      "train loss:0.999451610783622\n",
      "train loss:0.9897300152097455\n",
      "train loss:1.0810690463422536\n",
      "train loss:0.9686187449952844\n",
      "train loss:1.0345722829801831\n",
      "train loss:0.9007300693116271\n",
      "train loss:0.9088658858473704\n",
      "train loss:1.0393934458518919\n",
      "train loss:0.7868773498151366\n",
      "train loss:1.0611676435993596\n",
      "train loss:1.01025716320237\n",
      "train loss:0.9195163264542738\n",
      "train loss:0.7915581604206703\n",
      "train loss:0.9541920979467702\n",
      "train loss:0.928839857701776\n",
      "train loss:0.965767085385279\n",
      "train loss:0.9870072151362943\n",
      "train loss:0.9521444852698082\n",
      "train loss:0.8803357715122963\n",
      "train loss:0.9422730505698083\n",
      "train loss:1.0145076839907687\n",
      "train loss:0.8383328342005127\n",
      "train loss:0.9748157885972077\n",
      "train loss:0.924924024411748\n",
      "train loss:1.0736319247946988\n",
      "train loss:1.1404455466210037\n",
      "train loss:1.1617991658585185\n",
      "train loss:0.8335797773356518\n",
      "train loss:1.0070594396419204\n",
      "train loss:0.8423238044251513\n",
      "train loss:0.8071056450724754\n",
      "train loss:1.0559451566201543\n",
      "train loss:0.7613913877678113\n",
      "train loss:1.045167781627817\n",
      "train loss:0.982865300786961\n",
      "train loss:0.8466399682735147\n",
      "train loss:0.9501789132578712\n",
      "train loss:1.0159812398738646\n",
      "train loss:1.0062695564148163\n",
      "train loss:0.9040833372022642\n",
      "train loss:1.07909588470788\n",
      "train loss:0.9724581472694654\n",
      "train loss:0.9226106940941868\n",
      "train loss:1.0558788418037965\n",
      "train loss:0.8956682072665032\n",
      "train loss:0.9750100579638442\n",
      "train loss:0.9653393872318925\n",
      "train loss:1.0626386447112952\n",
      "train loss:0.9327126481755027\n",
      "train loss:1.1685330297577836\n",
      "train loss:1.0372248257501078\n",
      "train loss:0.9907569718134579\n",
      "train loss:1.1043272052313284\n",
      "train loss:0.933777794772228\n",
      "train loss:0.9404198870835642\n",
      "train loss:1.088742481468522\n",
      "train loss:0.922679886866715\n",
      "train loss:1.043838634018137\n",
      "train loss:0.7689732824262215\n",
      "train loss:0.9372383411040023\n",
      "train loss:1.0204395614958919\n",
      "train loss:0.9188742346525132\n",
      "train loss:1.026026612006204\n",
      "train loss:1.0425087686400776\n",
      "train loss:0.9603255050892269\n",
      "train loss:0.8991843630330548\n",
      "train loss:0.939653115167848\n",
      "train loss:0.9109452378917128\n",
      "train loss:0.9022946791091923\n",
      "train loss:0.9533405052762113\n",
      "train loss:0.8946777458035412\n",
      "train loss:0.9389071737815747\n",
      "train loss:0.8581087634614817\n",
      "train loss:0.9787329752922763\n",
      "train loss:0.8551559701688407\n",
      "train loss:0.9302754598336743\n",
      "train loss:1.235384344366626\n",
      "train loss:1.021356313719195\n",
      "train loss:0.8753623258813481\n",
      "train loss:0.7724031980751562\n",
      "train loss:0.9723473224373396\n",
      "train loss:1.0724902648828216\n",
      "train loss:0.9446282858755249\n",
      "train loss:0.9612931430853604\n",
      "train loss:0.8917691808854346\n",
      "train loss:0.9571719237893018\n",
      "train loss:0.8777531770192033\n",
      "train loss:1.0312902258399932\n",
      "train loss:0.999369450442359\n",
      "train loss:1.0387596250066273\n",
      "train loss:0.9006010938266183\n",
      "train loss:0.9641727500542656\n",
      "train loss:0.8204127014149446\n",
      "train loss:0.8460411652521205\n",
      "train loss:0.884930409941792\n",
      "train loss:0.9981286535806511\n",
      "train loss:1.0682497492261134\n",
      "train loss:1.0377750767746723\n",
      "train loss:0.9482960218377195\n",
      "train loss:0.991601284699242\n",
      "train loss:1.0345573509537942\n",
      "train loss:0.9952090214789089\n",
      "train loss:0.7822047601339195\n",
      "train loss:0.9653316065371575\n",
      "train loss:1.2020336686873145\n",
      "train loss:0.9422861529525135\n",
      "train loss:0.9049871903017858\n",
      "train loss:0.8222163883174401\n",
      "train loss:0.8803785161035473\n",
      "train loss:1.0061075217474649\n",
      "train loss:1.0419519382913465\n",
      "train loss:0.9967225139129213\n",
      "train loss:0.7619398421025378\n",
      "train loss:0.9731007177358818\n",
      "train loss:1.0819763206256159\n",
      "train loss:0.9261084266854871\n",
      "train loss:1.0443443609073952\n",
      "train loss:1.0264480416455508\n",
      "train loss:1.1040213958719354\n",
      "train loss:0.8722478462734202\n",
      "train loss:1.0061087358172514\n",
      "train loss:0.9397122125674119\n",
      "train loss:1.1247371843400262\n",
      "train loss:0.9516556076619479\n",
      "train loss:0.9581116419390144\n",
      "train loss:0.8735371933556765\n",
      "train loss:1.0263668845658709\n",
      "train loss:0.899618239791604\n",
      "train loss:1.0354839313095694\n",
      "train loss:1.1481391982341538\n",
      "train loss:0.8405176462651418\n",
      "train loss:1.1022015169548478\n",
      "train loss:1.0053519858532933\n",
      "train loss:1.0053227305499037\n",
      "train loss:0.9479991720364117\n",
      "train loss:0.9307614005160211\n",
      "train loss:1.0384213041166632\n",
      "train loss:0.9663238381486093\n",
      "train loss:0.8108789515692912\n",
      "train loss:0.9248447696448252\n",
      "train loss:1.0811768623612654\n",
      "train loss:1.0781620344118885\n",
      "train loss:0.8919856447109644\n",
      "train loss:0.8711317964078467\n",
      "train loss:0.9403539000838768\n",
      "train loss:0.9460480142209486\n",
      "train loss:0.8694917289615509\n",
      "train loss:0.9913666438844411\n",
      "train loss:1.0058561229779635\n",
      "train loss:0.9065412226022473\n",
      "train loss:1.0910386178142624\n",
      "train loss:0.9399509133878613\n",
      "train loss:1.0709493625093371\n",
      "train loss:0.8791651183324397\n",
      "train loss:1.0439574622574477\n",
      "train loss:0.9214031761687845\n",
      "train loss:0.8230091550380098\n",
      "train loss:0.8342387304383786\n",
      "train loss:0.9592069388564426\n",
      "train loss:0.8545709518460667\n",
      "train loss:1.0608205798254877\n",
      "train loss:1.028537278524067\n",
      "train loss:0.8919639142385427\n",
      "train loss:0.8019117017115049\n",
      "train loss:1.2175790319065734\n",
      "train loss:0.9403850932700085\n",
      "train loss:0.8741122935620793\n",
      "train loss:0.9113216781912672\n",
      "train loss:1.099178478116316\n",
      "train loss:0.9230807596885663\n",
      "train loss:0.9739927027896438\n",
      "train loss:0.9098479803450146\n",
      "train loss:1.0879801723332152\n",
      "train loss:0.990907249644995\n",
      "train loss:0.8870822702131663\n",
      "train loss:1.0686664128997707\n",
      "train loss:0.9566227583346102\n",
      "train loss:0.8683156493289437\n",
      "train loss:0.9796486212034494\n",
      "train loss:0.7732979872215116\n",
      "train loss:0.9791990966564748\n",
      "train loss:0.8014098399509648\n",
      "train loss:0.8122386775761352\n",
      "train loss:1.0025306917790204\n",
      "train loss:0.9492687583858472\n",
      "train loss:0.8116976444115994\n",
      "train loss:1.2009145958288894\n",
      "train loss:0.9186571789301\n",
      "train loss:0.8694524363886174\n",
      "train loss:0.9147729558825065\n",
      "train loss:0.9360760590930063\n",
      "train loss:0.8214706024426451\n",
      "train loss:1.0127424588310745\n",
      "train loss:0.9061484347657307\n",
      "train loss:1.0376542723678222\n",
      "train loss:1.0795376066875924\n",
      "train loss:0.9038043612355706\n",
      "train loss:0.8317478081217077\n",
      "train loss:1.026304264500162\n",
      "train loss:1.1225680350291483\n",
      "train loss:1.0953636303916596\n",
      "train loss:1.064621730009711\n",
      "train loss:0.8602724252693004\n",
      "train loss:0.8678361429339563\n",
      "train loss:0.9593363849418409\n",
      "train loss:0.8899060857114415\n",
      "train loss:0.8627194347266528\n",
      "train loss:0.8989675966588716\n",
      "train loss:0.9649680000309852\n",
      "train loss:0.979003523308922\n",
      "train loss:0.9883786361261961\n",
      "train loss:0.941176217552153\n",
      "train loss:0.9064260671731416\n",
      "train loss:1.0433671046240551\n",
      "train loss:0.9808423600478168\n",
      "train loss:1.0642210437287196\n",
      "train loss:1.0681033768114505\n",
      "train loss:0.9544017165186185\n",
      "train loss:0.8731049855469238\n",
      "train loss:0.8708359193748696\n",
      "train loss:0.9051166907528764\n",
      "train loss:0.8762250110748876\n",
      "train loss:0.9389090338974745\n",
      "train loss:0.9596483658534001\n",
      "train loss:0.9649348301922052\n",
      "train loss:1.0301236629229313\n",
      "train loss:0.9279039666146522\n",
      "train loss:0.9531219649150933\n",
      "train loss:1.0071377747167438\n",
      "train loss:0.8423567542774895\n",
      "train loss:1.0721676268444769\n",
      "train loss:1.0793711507793733\n",
      "train loss:0.9413655846980709\n",
      "train loss:1.053615483838995\n",
      "train loss:0.7875829748577768\n",
      "train loss:1.0031630568186327\n",
      "train loss:0.9839081451916377\n",
      "train loss:0.9178854220598978\n",
      "train loss:1.0701752972687342\n",
      "train loss:0.9778015439725377\n",
      "train loss:1.0160604125269819\n",
      "train loss:0.8831916399655289\n",
      "train loss:0.7757876428319834\n",
      "train loss:0.970926190263045\n",
      "train loss:0.8694664269295825\n",
      "train loss:0.7771290848330412\n",
      "train loss:0.9436707205656206\n",
      "train loss:1.1931875425734615\n",
      "train loss:1.011627410405047\n",
      "train loss:0.9111835341492626\n",
      "train loss:1.0657264388743681\n",
      "train loss:0.8446859727192659\n",
      "train loss:0.8653636522252056\n",
      "train loss:0.8338301799919232\n",
      "train loss:0.9491240300572134\n",
      "train loss:0.8446380617123895\n",
      "train loss:0.9531417376340592\n",
      "train loss:0.9466523668601728\n",
      "train loss:0.9402834102128582\n",
      "train loss:0.941779190846716\n",
      "train loss:0.9060867090729249\n",
      "train loss:0.9171662097263844\n",
      "train loss:0.8430871005723867\n",
      "train loss:1.0329302054468268\n",
      "train loss:0.9040240725133231\n",
      "train loss:0.8827390638329562\n",
      "train loss:1.0022565437917432\n",
      "train loss:0.8399847827376072\n",
      "train loss:0.8422320978084269\n",
      "train loss:0.789378319981851\n",
      "train loss:0.9804409478672943\n",
      "train loss:0.9068155490243707\n",
      "train loss:1.1935420589390662\n",
      "train loss:0.9096954857766003\n",
      "train loss:0.8971320242655857\n",
      "train loss:0.7730211607071853\n",
      "train loss:0.927410301389253\n",
      "train loss:0.8706692885729725\n",
      "train loss:1.1174124512140484\n",
      "train loss:0.9526184908080323\n",
      "train loss:0.7857348710008288\n",
      "train loss:0.8239203870418472\n",
      "train loss:1.0513665690612601\n",
      "train loss:1.182021780178018\n",
      "train loss:1.0678393405241369\n",
      "train loss:1.0191441917452086\n",
      "train loss:0.9812806952626818\n",
      "train loss:1.048414393174628\n",
      "train loss:1.0413783600011501\n",
      "train loss:0.9965043292464761\n",
      "train loss:0.9601678549530631\n",
      "train loss:1.042015292827025\n",
      "train loss:0.9833016077323775\n",
      "train loss:0.8567953771734054\n",
      "train loss:1.0039074863420963\n",
      "train loss:1.062968270869061\n",
      "train loss:1.149306076789415\n",
      "train loss:1.0061453705227785\n",
      "train loss:0.9472326538843455\n",
      "train loss:0.9010361047239006\n",
      "train loss:0.8783663595617991\n",
      "train loss:1.1395724297741254\n",
      "train loss:0.848807988223627\n",
      "train loss:1.0350610012675567\n",
      "train loss:0.9420497958761341\n",
      "train loss:1.0420935298171428\n",
      "train loss:0.9411107289131273\n",
      "train loss:0.874915666615556\n",
      "train loss:0.9680739934555125\n",
      "train loss:1.1398674803942157\n",
      "train loss:0.8974078129002321\n",
      "train loss:0.960083316338997\n",
      "train loss:0.8847571260702073\n",
      "train loss:0.9454628806664934\n",
      "train loss:1.0717591148760715\n",
      "train loss:1.2277171860758083\n",
      "train loss:1.0131204776086977\n",
      "train loss:0.9768114952386463\n",
      "train loss:0.986596609916209\n",
      "train loss:0.8477063453892978\n",
      "train loss:0.9027193473377079\n",
      "train loss:0.9452080401969722\n",
      "train loss:0.9186090620912942\n",
      "train loss:1.0502196265191228\n",
      "train loss:0.9945310894482137\n",
      "train loss:1.020978234045365\n",
      "train loss:1.0542271176515308\n",
      "train loss:1.0144371124800577\n",
      "train loss:1.070986036235373\n",
      "train loss:0.9183584856262934\n",
      "train loss:0.9545842679947247\n",
      "train loss:1.0594100785771097\n",
      "train loss:0.8376219631648051\n",
      "train loss:0.8967487787249431\n",
      "train loss:0.860631124406883\n",
      "train loss:0.9922444313450447\n",
      "train loss:0.9514926878241309\n",
      "train loss:1.0479849623365964\n",
      "train loss:0.8575603322782492\n",
      "train loss:1.1252973865689364\n",
      "train loss:0.9747364602920372\n",
      "train loss:1.0064000174232342\n",
      "train loss:0.9443150518054793\n",
      "train loss:0.9356421540253954\n",
      "train loss:0.9157996860917446\n",
      "train loss:1.0436249828792297\n",
      "train loss:1.0268199184189835\n",
      "train loss:0.9205981916985092\n",
      "train loss:0.9255064386069494\n",
      "train loss:0.9308094447776355\n",
      "train loss:1.1102202718756375\n",
      "train loss:1.1017315487125428\n",
      "train loss:0.9214194654973388\n",
      "train loss:0.9209207258971316\n",
      "train loss:0.7237416820692786\n",
      "train loss:0.9480295962027979\n",
      "train loss:0.9123886489104746\n",
      "train loss:0.8817042685026838\n",
      "train loss:0.9455068554016463\n",
      "train loss:0.9172840483210849\n",
      "train loss:1.0224630441995588\n",
      "train loss:0.8330335701778356\n",
      "train loss:1.0270396351805235\n",
      "train loss:0.766598955125038\n",
      "train loss:0.9166019129886247\n",
      "train loss:1.0696619411465746\n",
      "train loss:0.8563485885855334\n",
      "train loss:0.8856943864970329\n",
      "train loss:1.016720640542158\n",
      "train loss:1.0180733988595039\n",
      "train loss:1.0090346266222867\n",
      "train loss:0.8722802573795273\n",
      "train loss:0.9771091293547485\n",
      "train loss:0.8972787786750149\n",
      "train loss:1.0209962368633883\n",
      "train loss:0.9563309894125186\n",
      "train loss:0.9140638159583407\n",
      "train loss:1.0767610398718828\n",
      "train loss:0.9299344507709791\n",
      "train loss:1.0388297781218865\n",
      "train loss:1.1483351312517036\n",
      "train loss:0.888902257860166\n",
      "train loss:0.9600925941824938\n",
      "train loss:0.9169941856821616\n",
      "train loss:0.9409423008583044\n",
      "train loss:1.00283143360653\n",
      "train loss:0.7529347224422822\n",
      "train loss:0.7938029364227092\n",
      "train loss:1.0257793990752402\n",
      "train loss:1.076079483828504\n",
      "train loss:1.001720665875658\n",
      "train loss:1.0265112668767176\n",
      "train loss:0.8831883006261081\n",
      "train loss:1.1698948549943393\n",
      "train loss:0.8765277619449974\n",
      "train loss:0.9862419471995834\n",
      "train loss:1.0665541657770488\n",
      "train loss:1.036738391066769\n",
      "train loss:1.0104602637503317\n",
      "train loss:0.942160711831799\n",
      "train loss:1.0536475710573527\n",
      "train loss:0.9611641613341659\n",
      "train loss:0.9555398151271882\n",
      "train loss:0.829937739641633\n",
      "train loss:0.9152087883444832\n",
      "train loss:0.9715421816845747\n",
      "train loss:0.936913717591858\n",
      "train loss:1.1113033410445279\n",
      "train loss:1.0140867491243268\n",
      "train loss:1.0953369990805164\n",
      "train loss:0.9884129100075735\n",
      "train loss:0.8428434075602952\n",
      "train loss:0.9746580104502813\n",
      "train loss:1.0097114904119726\n",
      "train loss:0.8749947520239043\n",
      "train loss:0.9242356197950342\n",
      "train loss:1.1200683540005003\n",
      "train loss:0.8905339052695713\n",
      "train loss:1.0918779918764416\n",
      "train loss:1.2594104980314351\n",
      "train loss:1.071417958074337\n",
      "train loss:1.2029731217974962\n",
      "train loss:1.0578464136706092\n",
      "train loss:1.0330352140083532\n",
      "train loss:0.9068109386281299\n",
      "train loss:0.8665274428163507\n",
      "train loss:0.8819000502485497\n",
      "train loss:0.8564241783698036\n",
      "train loss:0.9324023622312031\n",
      "train loss:1.0964549185385395\n",
      "train loss:1.1266310404491118\n",
      "train loss:0.939591704231718\n",
      "train loss:0.9656244331422935\n",
      "train loss:1.1046637637433505\n",
      "train loss:1.1606798345254905\n",
      "train loss:0.9864934113531031\n",
      "train loss:0.9147938291675247\n",
      "train loss:0.8710667551115182\n",
      "train loss:1.0479391775436675\n",
      "train loss:0.9198564267654048\n",
      "train loss:0.8714452992302749\n",
      "train loss:0.9221118363750562\n",
      "train loss:1.0078311846350994\n",
      "train loss:1.063201193853196\n",
      "train loss:0.9338383572005108\n",
      "train loss:1.0122210524269792\n",
      "train loss:0.9417890497418204\n",
      "train loss:1.0205238905991294\n",
      "train loss:1.1279457178653514\n",
      "train loss:1.1359999109847632\n",
      "train loss:1.0693583987639872\n",
      "train loss:1.0941097074616368\n",
      "train loss:0.8847914435991692\n",
      "train loss:1.1964809867981685\n",
      "train loss:0.969738680913354\n",
      "=== epoch:3, train acc:0.986, test acc:0.974 ===\n",
      "train loss:0.8982106284401269\n",
      "train loss:0.9014424892419071\n",
      "train loss:0.9952203822657171\n",
      "train loss:0.9236539093194375\n",
      "train loss:1.0347934749665324\n",
      "train loss:0.9007132565069631\n",
      "train loss:0.9508972897943437\n",
      "train loss:0.7216799015979202\n",
      "train loss:0.916630450275487\n",
      "train loss:0.7987503135682972\n",
      "train loss:0.9944378760255406\n",
      "train loss:0.9319346423457213\n",
      "train loss:0.9714981961310896\n",
      "train loss:0.8171656584396959\n",
      "train loss:0.8506265602458296\n",
      "train loss:1.0288729730709323\n",
      "train loss:1.0603127106910726\n",
      "train loss:0.8638874600763958\n",
      "train loss:0.9520937433316725\n",
      "train loss:0.9549386438080993\n",
      "train loss:0.847864104664302\n",
      "train loss:1.0355698541911789\n",
      "train loss:0.769539925976804\n",
      "train loss:1.205961323682713\n",
      "train loss:1.1696569160220434\n",
      "train loss:0.8785145344954561\n",
      "train loss:1.013727782271488\n",
      "train loss:0.9361037883746587\n",
      "train loss:0.8360635437569399\n",
      "train loss:0.7937344912693466\n",
      "train loss:0.8779832749306447\n",
      "train loss:0.9631337581978576\n",
      "train loss:0.8672836749486187\n",
      "train loss:0.9805262875090613\n",
      "train loss:0.905106428928748\n",
      "train loss:0.9641968031626412\n",
      "train loss:0.9790231402331474\n",
      "train loss:0.7660600609610225\n",
      "train loss:1.0937646184909184\n",
      "train loss:1.0467119378308625\n",
      "train loss:0.9155378379915056\n",
      "train loss:0.9889970587670623\n",
      "train loss:1.1635818397504691\n",
      "train loss:0.95985974116648\n",
      "train loss:0.9833251184672162\n",
      "train loss:0.9151619161022283\n",
      "train loss:1.1080746578175136\n",
      "train loss:0.8123929813930983\n",
      "train loss:0.9859578324674882\n",
      "train loss:0.9660578473412361\n",
      "train loss:0.9221862608912916\n",
      "train loss:1.0347866637521155\n",
      "train loss:1.0042261254661504\n",
      "train loss:1.0080071718659027\n",
      "train loss:1.1102427212258537\n",
      "train loss:0.9316318244648447\n",
      "train loss:0.8990128832370279\n",
      "train loss:1.076082894929527\n",
      "train loss:0.8438799022149639\n",
      "train loss:0.8978578105805533\n",
      "train loss:1.1995913187803697\n",
      "train loss:1.0950025506688685\n",
      "train loss:1.0063760072084849\n",
      "train loss:0.9344697523273494\n",
      "train loss:0.8519129514098066\n",
      "train loss:0.9517819720212958\n",
      "train loss:1.0482980464464804\n",
      "train loss:0.9992051703671275\n",
      "train loss:0.8570470067151781\n",
      "train loss:0.918328331056142\n",
      "train loss:0.9893211793886978\n",
      "train loss:0.9028827653933046\n",
      "train loss:0.8314227023882108\n",
      "train loss:0.8230893855503738\n",
      "train loss:1.0028093181744084\n",
      "train loss:0.9348708897060731\n",
      "train loss:0.9148087036730719\n",
      "train loss:0.9593458829195401\n",
      "train loss:1.0804768453977314\n",
      "train loss:0.8074314403479937\n",
      "train loss:1.0013890968506263\n",
      "train loss:1.0223968958416234\n",
      "train loss:1.0175162292289215\n",
      "train loss:0.9132935252976379\n",
      "train loss:0.9533040211764164\n",
      "train loss:0.9985428370426611\n",
      "train loss:1.046808946577431\n",
      "train loss:1.1220260506570425\n",
      "train loss:0.9863450432759328\n",
      "train loss:1.0857323615946146\n",
      "train loss:1.0922063615403033\n",
      "train loss:0.8999392925933699\n",
      "train loss:0.8682122468004326\n",
      "train loss:0.9167564579505856\n",
      "train loss:1.016628923119753\n",
      "train loss:0.9846784280497949\n",
      "train loss:0.9048601112542852\n",
      "train loss:0.9320828083528825\n",
      "train loss:1.1198850538155376\n",
      "train loss:0.8609875819378229\n",
      "train loss:1.0672415386595402\n",
      "train loss:1.0557674355018178\n",
      "train loss:0.7770303270834623\n",
      "train loss:1.0457994292031882\n",
      "train loss:0.842123430357446\n",
      "train loss:0.9893975125430621\n",
      "train loss:1.0314080248788937\n",
      "train loss:0.900173697834839\n",
      "train loss:0.766764849351288\n",
      "train loss:0.9404376707630733\n",
      "train loss:0.8393825966655453\n",
      "train loss:0.9949890415886529\n",
      "train loss:0.8753483763909357\n",
      "train loss:0.9461893809999935\n",
      "train loss:1.032761022514779\n",
      "train loss:0.8344141538336403\n",
      "train loss:1.0509384529545907\n",
      "train loss:1.0457246518716155\n",
      "train loss:0.9091619039133616\n",
      "train loss:0.8864414481361844\n",
      "train loss:0.9575770362560055\n",
      "train loss:0.9214362426177728\n",
      "train loss:0.9731098266185088\n",
      "train loss:0.8325606350454513\n",
      "train loss:0.9249726123276565\n",
      "train loss:0.921782991093877\n",
      "train loss:0.7916726815612026\n",
      "train loss:0.9443069574123771\n",
      "train loss:0.885888409615277\n",
      "train loss:0.9265709157657379\n",
      "train loss:0.959535239586119\n",
      "train loss:1.0170038727208919\n",
      "train loss:0.8511174874615897\n",
      "train loss:0.9317749269999367\n",
      "train loss:0.7918172251169123\n",
      "train loss:0.8738947090866496\n",
      "train loss:0.8240037009073993\n",
      "train loss:0.9293529570748733\n",
      "train loss:0.9918126443609058\n",
      "train loss:1.0789257394642857\n",
      "train loss:0.9576190037906449\n",
      "train loss:1.0606043466263613\n",
      "train loss:0.9899708789969517\n",
      "train loss:0.9457962412857719\n",
      "train loss:1.0266064447335825\n",
      "train loss:0.9362566706288632\n",
      "train loss:1.153864570570448\n",
      "train loss:0.9340591601164954\n",
      "train loss:0.7928417745228066\n",
      "train loss:0.995738759334231\n",
      "train loss:0.9837005215079064\n",
      "train loss:0.996201033681427\n",
      "train loss:1.028894680964863\n",
      "train loss:0.9126281920898681\n",
      "train loss:0.850719114604762\n",
      "train loss:0.9365644370158204\n",
      "train loss:0.887698421883063\n",
      "train loss:0.9228162276351983\n",
      "train loss:0.9638025609689591\n",
      "train loss:0.9060668405719398\n",
      "train loss:1.0432719660218503\n",
      "train loss:0.8800384326299283\n",
      "train loss:0.8813999611078404\n",
      "train loss:0.9064603235289167\n",
      "train loss:0.9262919873170741\n",
      "train loss:0.9497923043721619\n",
      "train loss:0.9988075555149074\n",
      "train loss:0.9699621277225563\n",
      "train loss:0.9848427024962132\n",
      "train loss:0.9729957912694808\n",
      "train loss:1.0424370271430703\n",
      "train loss:1.0636127584199622\n",
      "train loss:0.8607161896657602\n",
      "train loss:0.9301571601295366\n",
      "train loss:1.1495652838198998\n",
      "train loss:1.1724897761529267\n",
      "train loss:1.006945686480382\n",
      "train loss:0.9102953556259193\n",
      "train loss:1.1808408503119443\n",
      "train loss:1.132275815933714\n",
      "train loss:0.8558709520383918\n",
      "train loss:0.8385588771586394\n",
      "train loss:1.021850467192844\n",
      "train loss:1.0650501753849992\n",
      "train loss:0.8761084120532974\n",
      "train loss:0.9882816075055193\n",
      "train loss:1.1317919984558928\n",
      "train loss:0.9394078567668163\n",
      "train loss:0.9738238829253025\n",
      "train loss:1.0969961204555871\n",
      "train loss:0.9951915132954771\n",
      "train loss:0.9173191172183074\n",
      "train loss:0.9722682021799003\n",
      "train loss:0.9613931313495823\n",
      "train loss:1.026993483823035\n",
      "train loss:1.08254842279829\n",
      "train loss:0.9872563556051382\n",
      "train loss:0.9559611396777256\n",
      "train loss:0.8762579866910035\n",
      "train loss:1.0497085138978466\n",
      "train loss:0.88391268414982\n",
      "train loss:0.9480789035866114\n",
      "train loss:0.8163369642978442\n",
      "train loss:0.8469587073203318\n",
      "train loss:0.8480457026872524\n",
      "train loss:0.819672669564715\n",
      "train loss:0.8198062717412852\n",
      "train loss:0.826797813235191\n",
      "train loss:0.8749624354293684\n",
      "train loss:1.0216775288583677\n",
      "train loss:0.8071763639666107\n",
      "train loss:0.8172060913711806\n",
      "train loss:0.9804549153342699\n",
      "train loss:0.9223832622731178\n",
      "train loss:1.0120881879120267\n",
      "train loss:0.9183132939075044\n",
      "train loss:0.9275870837274688\n",
      "train loss:1.0883500520191962\n",
      "train loss:1.1025570766398776\n",
      "train loss:0.9090286293482401\n",
      "train loss:0.9018104857076723\n",
      "train loss:0.9630824275213763\n",
      "train loss:0.8202230830945985\n",
      "train loss:0.9319750996771714\n",
      "train loss:0.9957726801385428\n",
      "train loss:1.0275649331375067\n",
      "train loss:1.1033970180440533\n",
      "train loss:0.8468761801892779\n",
      "train loss:1.001359960488444\n",
      "train loss:0.8027398700762004\n",
      "train loss:0.9933027574622842\n",
      "train loss:0.9869421263780724\n",
      "train loss:1.033312530541601\n",
      "train loss:0.9774956818844529\n",
      "train loss:1.055150098333409\n",
      "train loss:0.8750703325936371\n",
      "train loss:1.0850234201326698\n",
      "train loss:0.8916108081653944\n",
      "train loss:0.9799198633857306\n",
      "train loss:0.830220238300355\n",
      "train loss:1.0606768224669505\n",
      "train loss:0.9410668498462035\n",
      "train loss:0.7846337936029016\n",
      "train loss:0.9665658012340178\n",
      "train loss:1.0384024760529498\n",
      "train loss:0.787779430858373\n",
      "train loss:0.9722068933572552\n",
      "train loss:1.0245651842592483\n",
      "train loss:1.0251936568183837\n",
      "train loss:0.9982682222318104\n",
      "train loss:1.0677825136546693\n",
      "train loss:0.8305347240175197\n",
      "train loss:0.937255124651824\n",
      "train loss:1.001515153843392\n",
      "train loss:0.8241532050623738\n",
      "train loss:0.8809435021704449\n",
      "train loss:0.990158878279553\n",
      "train loss:0.8192972878571873\n",
      "train loss:0.899678296189367\n",
      "train loss:0.9258735744674549\n",
      "train loss:0.9691204639295644\n",
      "train loss:1.0038539410811114\n",
      "train loss:0.8667231614996344\n",
      "train loss:0.8751301807524564\n",
      "train loss:0.9907631887439168\n",
      "train loss:0.958544616643839\n",
      "train loss:0.8103030057785512\n",
      "train loss:0.787863101438536\n",
      "train loss:0.9193527523424356\n",
      "train loss:0.8986582646941492\n",
      "train loss:0.8055286736570549\n",
      "train loss:0.9387056643733714\n",
      "train loss:0.86584178098892\n",
      "train loss:1.1349706468303424\n",
      "train loss:0.9569838136883991\n",
      "train loss:0.9918518284563334\n",
      "train loss:0.9699019127880866\n",
      "train loss:0.8694127334545199\n",
      "train loss:0.8889032335067324\n",
      "train loss:0.9596861760622921\n",
      "train loss:0.7873922168869114\n",
      "train loss:0.8655176250964347\n",
      "train loss:0.964795771461352\n",
      "train loss:1.0743447859732294\n",
      "train loss:0.9030655214345359\n",
      "train loss:0.9508518543430459\n",
      "train loss:0.9111725597010464\n",
      "train loss:0.920617574763639\n",
      "train loss:0.9590640212188201\n",
      "train loss:1.120386797624641\n",
      "train loss:0.8919749797493179\n",
      "train loss:0.6841154501396554\n",
      "train loss:0.8768395244987643\n",
      "train loss:0.8980180961331454\n",
      "train loss:0.8296242143048762\n",
      "train loss:0.8817891541654145\n",
      "train loss:0.8353792076552442\n",
      "train loss:1.0838792350129802\n",
      "train loss:0.9794814929650102\n",
      "train loss:0.9435608306389447\n",
      "train loss:0.8838778227658368\n",
      "train loss:0.9940221961558434\n",
      "train loss:0.9765218409642047\n",
      "train loss:0.9389008109388145\n",
      "train loss:0.9008566907224552\n",
      "train loss:0.8541713745495153\n",
      "train loss:0.7558829017747035\n",
      "train loss:0.8274568257053042\n",
      "train loss:1.1306644256243747\n",
      "train loss:1.0199211442646927\n",
      "train loss:1.0362718961728021\n",
      "train loss:1.1420268671879297\n",
      "train loss:0.9656980132460794\n",
      "train loss:0.9787414658226439\n",
      "train loss:0.8624417368326192\n",
      "train loss:0.9859465820975766\n",
      "train loss:0.9468199067656555\n",
      "train loss:1.1371912638252633\n",
      "train loss:0.8642474198893345\n",
      "train loss:1.0693718063674036\n",
      "train loss:0.7909843790040021\n",
      "train loss:1.088909137363787\n",
      "train loss:0.8540767689907283\n",
      "train loss:0.8407769766849562\n",
      "train loss:0.960497583652305\n",
      "train loss:0.9687372994260379\n",
      "train loss:0.9285027348736081\n",
      "train loss:0.8911543351208762\n",
      "train loss:0.9511806916228358\n",
      "train loss:0.8430506783659752\n",
      "train loss:0.7341130960853464\n",
      "train loss:0.9890533903497466\n",
      "train loss:0.8270588030646828\n",
      "train loss:1.1104369818627347\n",
      "train loss:1.1360745368920304\n",
      "train loss:0.9769296945729145\n",
      "train loss:0.9143405173204268\n",
      "train loss:0.7605367880489087\n",
      "train loss:0.9530407487566868\n",
      "train loss:0.8729122955414992\n",
      "train loss:0.938070130735563\n",
      "train loss:0.8858494659239962\n",
      "train loss:0.8216551931544158\n",
      "train loss:0.9900748683481224\n",
      "train loss:0.9199156913167379\n",
      "train loss:1.0166746596491185\n",
      "train loss:1.0543114868196304\n",
      "train loss:0.9342979840351907\n",
      "train loss:1.0124816014008553\n",
      "train loss:1.011044422809442\n",
      "train loss:0.9152144748465288\n",
      "train loss:0.8959295961322116\n",
      "train loss:0.8407250010511705\n",
      "train loss:0.971220526317014\n",
      "train loss:0.8923708221052438\n",
      "train loss:0.7916268414824568\n",
      "train loss:0.7423179091115064\n",
      "train loss:0.8679293439827592\n",
      "train loss:0.8782485045185487\n",
      "train loss:0.9296406079103687\n",
      "train loss:0.9717514848637866\n",
      "train loss:0.9111959818633086\n",
      "train loss:0.7584042693844146\n",
      "train loss:0.7883844171624138\n",
      "train loss:0.9614604497796004\n",
      "train loss:0.8724788345784362\n",
      "train loss:0.869784728453739\n",
      "train loss:1.0192328957635848\n",
      "train loss:0.9144241420737076\n",
      "train loss:0.7135573026648178\n",
      "train loss:0.9539054230434653\n",
      "train loss:0.829309023336933\n",
      "train loss:0.9277210091764122\n",
      "train loss:0.7876790422074642\n",
      "train loss:0.9707740710203057\n",
      "train loss:0.9953064718805605\n",
      "train loss:0.9885807512288474\n",
      "train loss:1.033607740018273\n",
      "train loss:0.9224516640455915\n",
      "train loss:1.063981918595769\n",
      "train loss:0.9269605966071357\n",
      "train loss:1.009664647997407\n",
      "train loss:0.822311793901812\n",
      "train loss:0.8650444534574752\n",
      "train loss:0.8991502958005259\n",
      "train loss:0.679975423225115\n",
      "train loss:1.0050877863518606\n",
      "train loss:0.8750556369929413\n",
      "train loss:0.6750273699028901\n",
      "train loss:0.9918756725251915\n",
      "train loss:0.7720117629832498\n",
      "train loss:0.9749952665235153\n",
      "train loss:0.8948688395612767\n",
      "train loss:1.0041793011696103\n",
      "train loss:0.9440245551170965\n",
      "train loss:0.9426120117444744\n",
      "train loss:0.8847471130854505\n",
      "train loss:1.058898206977533\n",
      "train loss:0.9473258369153704\n",
      "train loss:0.9613689861068138\n",
      "train loss:0.9436850403389188\n",
      "train loss:0.8594185816005626\n",
      "train loss:0.7197574086305208\n",
      "train loss:0.8650230061925028\n",
      "train loss:1.000055998514579\n",
      "train loss:0.7857712488607039\n",
      "train loss:0.7944360878533807\n",
      "train loss:0.9456089991980136\n",
      "train loss:0.8525923724074201\n",
      "train loss:0.9986822292396864\n",
      "train loss:0.8333132115901427\n",
      "train loss:0.8978360706770527\n",
      "train loss:1.0900504416069419\n",
      "train loss:0.7145849296730072\n",
      "train loss:0.9130009018556617\n",
      "train loss:0.8440845051016352\n",
      "train loss:0.9017838853218759\n",
      "train loss:0.9967449952554768\n",
      "train loss:0.9012338399975859\n",
      "train loss:0.8684667519758765\n",
      "train loss:0.9423097764797076\n",
      "train loss:0.8183297946652007\n",
      "train loss:0.9126347877267116\n",
      "train loss:0.8595684814267855\n",
      "train loss:0.9634778265855486\n",
      "train loss:0.9594771197706505\n",
      "train loss:1.0153736080301148\n",
      "train loss:1.0131547743686289\n",
      "train loss:0.8448087120619175\n",
      "train loss:0.8596195102795879\n",
      "train loss:0.8588723252793495\n",
      "train loss:1.0278705607729104\n",
      "train loss:1.037347036767566\n",
      "train loss:0.9025743178535987\n",
      "train loss:0.8938991673830751\n",
      "train loss:0.8987646963834002\n",
      "train loss:1.0624747201939406\n",
      "train loss:0.9810378492132653\n",
      "train loss:0.9439860888482918\n",
      "train loss:0.872655846332162\n",
      "train loss:0.9112376976926093\n",
      "train loss:0.9392769418666053\n",
      "train loss:0.910222581338687\n",
      "train loss:0.9547939779646866\n",
      "train loss:0.8959789862441041\n",
      "train loss:0.8602547937110236\n",
      "train loss:1.0151170483511258\n",
      "train loss:0.7630758672691469\n",
      "train loss:0.8580402287843942\n",
      "train loss:0.9326758550729122\n",
      "train loss:0.924123613318224\n",
      "train loss:1.0216079023331242\n",
      "train loss:0.8875518025567763\n",
      "train loss:0.8958684062379354\n",
      "train loss:0.8225059018125046\n",
      "train loss:0.7528957127271376\n",
      "train loss:0.888377924724565\n",
      "train loss:1.0438407312334181\n",
      "train loss:0.959764812189308\n",
      "train loss:0.9474046493078369\n",
      "train loss:0.9118284444959968\n",
      "train loss:0.9538129160308586\n",
      "train loss:1.0558316820185392\n",
      "train loss:1.0488480139950185\n",
      "train loss:0.9929556589013656\n",
      "train loss:0.8931923103928715\n",
      "train loss:1.063186096004536\n",
      "train loss:1.0418240048394098\n",
      "train loss:1.05543943331232\n",
      "train loss:1.1069851924477938\n",
      "train loss:0.8714620101261787\n",
      "train loss:0.9185154370812278\n",
      "train loss:1.1101647148716807\n",
      "train loss:0.9379145153986473\n",
      "train loss:1.0566542580243163\n",
      "train loss:1.017336046736162\n",
      "train loss:0.9109256581342122\n",
      "train loss:0.844645275216465\n",
      "train loss:1.0785741401843756\n",
      "train loss:0.9419337809520992\n",
      "train loss:0.9317842860891339\n",
      "train loss:0.88573261530773\n",
      "train loss:0.8657363086320329\n",
      "train loss:1.2062197677986857\n",
      "train loss:0.6765664299034323\n",
      "train loss:0.9482445583492769\n",
      "train loss:1.1120088881498083\n",
      "train loss:1.0052049583934144\n",
      "train loss:1.0940564274762852\n",
      "train loss:0.9308560325341837\n",
      "train loss:0.8363324444623487\n",
      "train loss:0.8863654255671672\n",
      "train loss:0.8910926417977751\n",
      "train loss:0.9474123193655959\n",
      "train loss:0.8955764650461804\n",
      "train loss:0.722661149838616\n",
      "train loss:0.9475057935752088\n",
      "train loss:0.8049206227353721\n",
      "train loss:0.9078489775978317\n",
      "train loss:0.9899250564790925\n",
      "train loss:1.081237649402702\n",
      "train loss:0.8704718313152761\n",
      "train loss:0.8466643114410302\n",
      "train loss:0.9381578667490149\n",
      "train loss:1.069782261263506\n",
      "train loss:0.9367393150257735\n",
      "train loss:1.0278539335102614\n",
      "train loss:0.8687807146452066\n",
      "train loss:0.8090967087788533\n",
      "train loss:0.8204943911391667\n",
      "train loss:0.9453365532733681\n",
      "train loss:1.0409064282265301\n",
      "train loss:0.8648133213073117\n",
      "train loss:0.9923388740337229\n",
      "train loss:1.1070035109138792\n",
      "train loss:0.98457890457728\n",
      "train loss:0.7904896585715266\n",
      "train loss:0.8348052142899554\n",
      "train loss:1.0159591465040185\n",
      "train loss:0.969308493596884\n",
      "train loss:0.875325958178392\n",
      "train loss:0.8829128303808231\n",
      "train loss:1.0325126749944364\n",
      "train loss:1.0092380102385667\n",
      "train loss:0.8434818207006898\n",
      "train loss:1.0160077594287984\n",
      "train loss:0.7763730891484504\n",
      "train loss:0.8718631216025099\n",
      "train loss:0.8867912582702256\n",
      "train loss:0.8593481409905351\n",
      "train loss:0.9701515920098618\n",
      "train loss:0.9787360587010593\n",
      "train loss:0.8630376934783396\n",
      "train loss:0.9511122761725411\n",
      "train loss:0.8735730366828633\n",
      "train loss:1.2209145901158982\n",
      "train loss:0.9271660253070557\n",
      "train loss:1.0681412067490792\n",
      "train loss:0.7140049404420319\n",
      "train loss:0.8757941799467898\n",
      "train loss:0.7264647018736018\n",
      "train loss:0.8868069928130045\n",
      "train loss:0.8764692143784868\n",
      "train loss:0.9403831467505874\n",
      "train loss:1.068592306162231\n",
      "train loss:0.9658268319543111\n",
      "train loss:1.1018770274857042\n",
      "train loss:0.9659720056877635\n",
      "train loss:0.9601299072317832\n",
      "train loss:0.8597907381823602\n",
      "train loss:0.8668780354555468\n",
      "train loss:0.7748809096569534\n",
      "train loss:0.9870867595662881\n",
      "train loss:0.9086963188582391\n",
      "train loss:0.8909795610019765\n",
      "train loss:0.826894618968923\n",
      "train loss:0.859183711651416\n",
      "train loss:0.9578972417637233\n",
      "train loss:0.8302781136113693\n",
      "train loss:1.0656641570672252\n",
      "train loss:0.7932299563962512\n",
      "train loss:0.8680793965748679\n",
      "train loss:0.9405846263993234\n",
      "train loss:0.8994220909800145\n",
      "train loss:0.7942680550162797\n",
      "train loss:0.914758368119078\n",
      "train loss:1.0160566515961\n",
      "train loss:1.0065486065186058\n",
      "train loss:0.9041085179586532\n",
      "train loss:0.8729340882866629\n",
      "train loss:1.044466082527357\n",
      "train loss:0.8787606942034419\n",
      "train loss:0.9566935030057536\n",
      "train loss:1.0149069998832854\n",
      "train loss:0.776215046442416\n",
      "train loss:0.7308752625213092\n",
      "train loss:1.0967965259297991\n",
      "train loss:0.8599650067290086\n",
      "train loss:0.8630031022213949\n",
      "train loss:0.7964648348400875\n",
      "train loss:0.8723204392813202\n",
      "train loss:1.0303940395956206\n",
      "train loss:0.8482270742376624\n",
      "train loss:0.7809676813952222\n",
      "train loss:0.8186906332753208\n",
      "train loss:0.9868955770009635\n",
      "train loss:0.8823947093267374\n",
      "train loss:0.9491176229985477\n",
      "train loss:0.7551672999793105\n",
      "train loss:0.8532012542846934\n",
      "train loss:0.945725406891238\n",
      "train loss:0.9135114156862457\n",
      "train loss:0.9790689629789986\n",
      "train loss:0.8895177556326508\n",
      "train loss:0.7768613683929222\n",
      "train loss:0.8725616283953451\n",
      "train loss:0.8703506365857501\n",
      "train loss:0.9020584005787083\n",
      "train loss:0.8139614343087623\n",
      "train loss:0.802034822083637\n",
      "=== epoch:4, train acc:0.986, test acc:0.987 ===\n",
      "train loss:0.9559567331564012\n",
      "train loss:0.8923567250460266\n",
      "train loss:0.6942358037600495\n",
      "train loss:0.8611086399008688\n",
      "train loss:0.8997864994472168\n",
      "train loss:0.8016634478566719\n",
      "train loss:0.983755166320495\n",
      "train loss:1.1542219114416132\n",
      "train loss:0.894172822811226\n",
      "train loss:1.0411094980617575\n",
      "train loss:0.9893074136265332\n",
      "train loss:0.9302669609188697\n",
      "train loss:0.8810657437122466\n",
      "train loss:0.7912356659947666\n",
      "train loss:0.965758732935544\n",
      "train loss:0.8044475628209355\n",
      "train loss:0.8522490921920365\n",
      "train loss:0.879053505405784\n",
      "train loss:0.9251787837260916\n",
      "train loss:1.0698105327936516\n",
      "train loss:0.9678497295561086\n",
      "train loss:0.9948891053101699\n",
      "train loss:0.7652379963117533\n",
      "train loss:0.9664912514533914\n",
      "train loss:0.7581568359230136\n",
      "train loss:0.8973239602994663\n",
      "train loss:0.7756321946528777\n",
      "train loss:0.9148002270077061\n",
      "train loss:0.9016945295646077\n",
      "train loss:0.9073135530562165\n",
      "train loss:1.0102556615654772\n",
      "train loss:0.9548702574079936\n",
      "train loss:0.9018779746747903\n",
      "train loss:0.930484698683214\n",
      "train loss:1.1540798748056162\n",
      "train loss:0.868961746862668\n",
      "train loss:0.7309457396363527\n",
      "train loss:0.9300827918033376\n",
      "train loss:0.9152488440042377\n",
      "train loss:0.8912396318111127\n",
      "train loss:0.8865229196229576\n",
      "train loss:0.9066209919881295\n",
      "train loss:0.9681675855263788\n",
      "train loss:0.9358853215899984\n",
      "train loss:0.8643182618536237\n",
      "train loss:0.9611963264275342\n",
      "train loss:0.9732085456945816\n",
      "train loss:1.0007971908901534\n",
      "train loss:0.9429988981136723\n",
      "train loss:1.0344733642807358\n",
      "train loss:0.9741570633485962\n",
      "train loss:0.9349583679587447\n",
      "train loss:0.9019977994558442\n",
      "train loss:0.8601994963294685\n",
      "train loss:0.982556445781475\n",
      "train loss:0.6755611887830055\n",
      "train loss:1.0766619042650423\n",
      "train loss:0.93656539486968\n",
      "train loss:0.9825575197324673\n",
      "train loss:0.8190420509493965\n",
      "train loss:0.9474646110020859\n",
      "train loss:0.888489460189564\n",
      "train loss:0.9097245698330362\n",
      "train loss:0.8813946728416566\n",
      "train loss:0.7445156599131642\n",
      "train loss:0.9449439038725785\n",
      "train loss:0.9639000530459456\n",
      "train loss:1.074724978825723\n",
      "train loss:0.992529425117551\n",
      "train loss:0.9389717431943175\n",
      "train loss:0.9140074026466164\n",
      "train loss:1.0493355942748208\n",
      "train loss:0.9986053007401235\n",
      "train loss:0.8627327170013289\n",
      "train loss:0.9357449375066652\n",
      "train loss:0.9236617645496801\n",
      "train loss:0.8708855518616571\n",
      "train loss:0.759336051229912\n",
      "train loss:1.005953327790589\n",
      "train loss:0.8407452865159613\n",
      "train loss:0.8817785129452621\n",
      "train loss:0.9164297853278057\n",
      "train loss:0.9378235391917825\n",
      "train loss:1.0600148318843785\n",
      "train loss:1.038550094174524\n",
      "train loss:1.0122213022845157\n",
      "train loss:0.9307659239220455\n",
      "train loss:1.1067495205934674\n",
      "train loss:0.7899486586461694\n",
      "train loss:0.9264745262594815\n",
      "train loss:0.9028749991557596\n",
      "train loss:0.8311809749826484\n",
      "train loss:0.7792469838484003\n",
      "train loss:0.9260920533438686\n",
      "train loss:0.9249629290076666\n",
      "train loss:1.0512174494756732\n",
      "train loss:0.8930204774011371\n",
      "train loss:1.2688336099622441\n",
      "train loss:0.8677663734872277\n",
      "train loss:1.057212080360867\n",
      "train loss:0.9440371477015295\n",
      "train loss:1.0238227172023284\n",
      "train loss:0.8334506626340241\n",
      "train loss:0.7769856550043791\n",
      "train loss:0.7735771835536254\n",
      "train loss:0.878844919104958\n",
      "train loss:0.9594227879077916\n",
      "train loss:0.8023941736001444\n",
      "train loss:1.0403666071099271\n",
      "train loss:0.8759181583998275\n",
      "train loss:1.0160458724104626\n",
      "train loss:1.0816175637256367\n",
      "train loss:0.9585944205437273\n",
      "train loss:1.0206921932210418\n",
      "train loss:0.9145680391669657\n",
      "train loss:1.1606552784653212\n",
      "train loss:0.9610218236110423\n",
      "train loss:0.8989967670495074\n",
      "train loss:0.8867004465510051\n",
      "train loss:0.8682162285643692\n",
      "train loss:0.9528333235447295\n",
      "train loss:1.0842019205751585\n",
      "train loss:0.8931518973466953\n",
      "train loss:0.9340452596332856\n",
      "train loss:0.7520916924949083\n",
      "train loss:1.0529725974226887\n",
      "train loss:0.939818208627008\n",
      "train loss:0.9537370467740268\n",
      "train loss:0.9620760684747265\n",
      "train loss:1.041620369793416\n",
      "train loss:0.9699757544647147\n",
      "train loss:0.9506935276574184\n",
      "train loss:0.8285298280912037\n",
      "train loss:0.9589821271042136\n",
      "train loss:0.8577495420842041\n",
      "train loss:0.8716995427806247\n",
      "train loss:0.9481423516710207\n",
      "train loss:0.8712499882114699\n",
      "train loss:0.891940179701742\n",
      "train loss:1.0199565883415844\n",
      "train loss:0.9457534623823534\n",
      "train loss:0.974240898364956\n",
      "train loss:0.9707036472060909\n",
      "train loss:1.1867630756494219\n",
      "train loss:0.8439991196461912\n",
      "train loss:0.7251694647539414\n",
      "train loss:0.8856304051221444\n",
      "train loss:1.125585111365533\n",
      "train loss:0.8904012125601243\n",
      "train loss:0.9249053639304938\n",
      "train loss:1.0026106044558925\n",
      "train loss:0.9314186335189631\n",
      "train loss:0.986384781906877\n",
      "train loss:1.1130761504126319\n",
      "train loss:0.963056944952083\n",
      "train loss:0.8799408737540049\n",
      "train loss:0.9221114752023044\n",
      "train loss:0.9631706767916842\n",
      "train loss:0.8422346340269197\n",
      "train loss:1.0253833659519644\n",
      "train loss:0.9619019309130603\n",
      "train loss:0.8453679167395604\n",
      "train loss:0.8782537363528753\n",
      "train loss:0.7149287571865284\n",
      "train loss:1.0177594169389783\n",
      "train loss:0.9438672662826574\n",
      "train loss:0.9553364423779169\n",
      "train loss:0.8499009895611084\n",
      "train loss:0.8424282936538348\n",
      "train loss:0.8777471989238589\n",
      "train loss:0.9528682855906436\n",
      "train loss:0.9481612968299779\n",
      "train loss:0.916023379977456\n",
      "train loss:1.0400900802915722\n",
      "train loss:0.8209812749114981\n",
      "train loss:0.8716564137596938\n",
      "train loss:0.9728390780657905\n",
      "train loss:1.1399783510436\n",
      "train loss:0.9465823380625517\n",
      "train loss:0.8314580664892559\n",
      "train loss:1.01412466529964\n",
      "train loss:0.8412829182292768\n",
      "train loss:0.7626873731453653\n",
      "train loss:1.0799862781338865\n",
      "train loss:0.8648959014061797\n",
      "train loss:0.9385113351260176\n",
      "train loss:1.0837767553276096\n",
      "train loss:1.0371572719026418\n",
      "train loss:0.9214489299310514\n",
      "train loss:0.9208632929003879\n",
      "train loss:0.9914508302511357\n",
      "train loss:1.0088229850833492\n",
      "train loss:0.9323378528626138\n",
      "train loss:0.8731315593472444\n",
      "train loss:0.8364993576852956\n",
      "train loss:0.8656884835597622\n",
      "train loss:0.918291796294226\n",
      "train loss:0.87990194256162\n",
      "train loss:0.8686023924319854\n",
      "train loss:0.9521746456194434\n",
      "train loss:1.0575174584679907\n",
      "train loss:1.064555954116107\n",
      "train loss:1.1040628027969623\n",
      "train loss:0.9085936440315172\n",
      "train loss:0.8533466142785068\n",
      "train loss:0.7319115934343309\n",
      "train loss:0.9337820295853859\n",
      "train loss:0.9033089181837458\n",
      "train loss:0.9894112995508152\n",
      "train loss:0.8645893121288287\n",
      "train loss:0.8419009800195729\n",
      "train loss:1.0112265602335904\n",
      "train loss:0.9278054070796362\n",
      "train loss:0.9810586287221347\n",
      "train loss:0.9446346070556468\n",
      "train loss:0.8769516933238137\n",
      "train loss:1.0183204230640583\n",
      "train loss:0.8448158340327183\n",
      "train loss:0.9939431756168392\n",
      "train loss:0.8121069890037049\n",
      "train loss:1.017684792114633\n",
      "train loss:0.7543159818083196\n",
      "train loss:1.0429829542670443\n",
      "train loss:0.9170796851361039\n",
      "train loss:1.0986397213870185\n",
      "train loss:0.9523839704046275\n",
      "train loss:0.9688859467154465\n",
      "train loss:0.9237720677776725\n",
      "train loss:0.9406240054969601\n",
      "train loss:0.9334973512062169\n",
      "train loss:1.0614095782567852\n",
      "train loss:0.7986500047279088\n",
      "train loss:0.9229461769320926\n",
      "train loss:0.8840591937439561\n",
      "train loss:0.9647147512312786\n",
      "train loss:0.8588834254683904\n",
      "train loss:0.9943487704783717\n",
      "train loss:1.0517492298808713\n",
      "train loss:0.897279276861014\n",
      "train loss:0.9597918215830882\n",
      "train loss:0.9003673895680779\n",
      "train loss:1.1299566996973156\n",
      "train loss:0.8468240091832681\n",
      "train loss:0.9571022990040688\n",
      "train loss:0.8846935247389779\n",
      "train loss:0.8659324451061517\n",
      "train loss:0.8505240203710556\n",
      "train loss:0.8566315173430022\n",
      "train loss:0.9103558576312701\n",
      "train loss:0.9085571309248267\n",
      "train loss:1.0228549335198167\n",
      "train loss:1.0135213707251092\n",
      "train loss:0.8169208868897617\n",
      "train loss:0.984824343389614\n",
      "train loss:0.7795134678648142\n",
      "train loss:0.7761534555645103\n",
      "train loss:1.0405151612771906\n",
      "train loss:1.020500564969473\n",
      "train loss:1.0488466748255254\n",
      "train loss:0.7827733108184435\n",
      "train loss:0.8031852147225946\n",
      "train loss:1.0754010950271273\n",
      "train loss:0.9932532376351233\n",
      "train loss:0.9670113031728284\n",
      "train loss:0.77103088833106\n",
      "train loss:0.7415019391455187\n",
      "train loss:0.8958462361735328\n",
      "train loss:0.7204720980619942\n",
      "train loss:0.9184162299155746\n",
      "train loss:0.9109502930801442\n",
      "train loss:0.958539845711408\n",
      "train loss:0.924189134590607\n",
      "train loss:0.8112572501895693\n",
      "train loss:1.1028516272464122\n",
      "train loss:0.9618282652975565\n",
      "train loss:0.9598423959263735\n",
      "train loss:0.9628744743093983\n",
      "train loss:0.7374212661460358\n",
      "train loss:0.8793797195409649\n",
      "train loss:0.7452109789614794\n",
      "train loss:0.780285050687949\n",
      "train loss:0.9138393681223578\n",
      "train loss:0.7851992466375508\n",
      "train loss:0.8896101613804623\n",
      "train loss:0.8935371340898521\n",
      "train loss:0.9521229312866673\n",
      "train loss:0.8758448039880115\n",
      "train loss:0.8739982551748369\n",
      "train loss:0.9311555807489604\n",
      "train loss:0.8838796506739836\n",
      "train loss:0.7911478047681877\n",
      "train loss:1.0011493038436872\n",
      "train loss:0.8569863601848351\n",
      "train loss:0.8786372711697479\n",
      "train loss:0.8188416814625987\n",
      "train loss:0.9435711324333311\n",
      "train loss:0.8540196769554447\n",
      "train loss:0.9358777155225851\n",
      "train loss:0.9341671023922496\n",
      "train loss:0.8490902034385615\n",
      "train loss:1.0875821408551616\n",
      "train loss:0.9270852889296981\n",
      "train loss:0.8665715076365558\n",
      "train loss:1.1366942702618545\n",
      "train loss:0.8396954432192167\n",
      "train loss:0.990402259401581\n",
      "train loss:0.8886227916717974\n",
      "train loss:0.9007620490824496\n",
      "train loss:0.7770243248795328\n",
      "train loss:0.8785761385696448\n",
      "train loss:0.8755963886369663\n",
      "train loss:0.9608864352197668\n",
      "train loss:0.927728930112209\n",
      "train loss:1.0563907835350321\n",
      "train loss:0.7881374618988565\n",
      "train loss:0.8008857783933672\n",
      "train loss:1.0633752009243087\n",
      "train loss:1.0298875576836395\n",
      "train loss:0.9816518283368907\n",
      "train loss:0.9843298845170814\n",
      "train loss:0.9649284167396952\n",
      "train loss:0.7608898758295872\n",
      "train loss:0.811244033576587\n",
      "train loss:0.8575730991484156\n",
      "train loss:0.7728440699385399\n",
      "train loss:1.0049665378515218\n",
      "train loss:0.8888170615028078\n",
      "train loss:0.7397304390223415\n",
      "train loss:1.0077128277893783\n",
      "train loss:0.9136886833117656\n",
      "train loss:0.8222985408962529\n",
      "train loss:0.8952333313576425\n",
      "train loss:1.0093805681161383\n",
      "train loss:0.9114866834027845\n",
      "train loss:0.8312901462381855\n",
      "train loss:0.9315917442457312\n",
      "train loss:0.7816744014694271\n",
      "train loss:0.9716445743743772\n",
      "train loss:0.9961194774778143\n",
      "train loss:0.9575863915995295\n",
      "train loss:0.938337557503993\n",
      "train loss:1.0021599712968419\n",
      "train loss:0.9624838527168262\n",
      "train loss:0.9198316435515156\n",
      "train loss:1.0475867048208014\n",
      "train loss:0.8360877785584413\n",
      "train loss:0.9317003551209561\n",
      "train loss:0.7975426064285304\n",
      "train loss:0.8574029843012957\n",
      "train loss:1.0523313038123148\n",
      "train loss:1.0045901915422357\n",
      "train loss:0.9822574014466585\n",
      "train loss:0.9632079617906633\n",
      "train loss:0.8724839036487139\n",
      "train loss:0.8920462649062378\n",
      "train loss:0.9011228751271376\n",
      "train loss:0.9357636348683505\n",
      "train loss:0.8681834254794234\n",
      "train loss:0.9515816924557634\n",
      "train loss:1.0067603691598046\n",
      "train loss:0.9374012141206268\n",
      "train loss:0.9162442040155837\n",
      "train loss:1.0158499410620825\n",
      "train loss:0.9299720421204192\n",
      "train loss:0.8367890055653389\n",
      "train loss:1.0950397147598272\n",
      "train loss:0.8991104497058466\n",
      "train loss:0.8820777948522831\n",
      "train loss:0.9348193285932733\n",
      "train loss:0.8543978820710478\n",
      "train loss:0.8971961797396847\n",
      "train loss:0.9349503579258993\n",
      "train loss:1.0016497615184794\n",
      "train loss:0.8953452579122184\n",
      "train loss:0.7021192313528186\n",
      "train loss:0.9359425861472846\n",
      "train loss:1.0154100269018165\n",
      "train loss:0.8927966629015915\n",
      "train loss:0.733534715904289\n",
      "train loss:0.8625825468719209\n",
      "train loss:0.9762505804602821\n",
      "train loss:0.9173942092560058\n",
      "train loss:0.8271061316258898\n",
      "train loss:0.8850875761962055\n",
      "train loss:0.9224317229376189\n",
      "train loss:0.9203669415166368\n",
      "train loss:1.074500165836293\n",
      "train loss:0.8717501892278464\n",
      "train loss:0.827404355600138\n",
      "train loss:0.9623866969215134\n",
      "train loss:0.8953833479231725\n",
      "train loss:0.9730582981273003\n",
      "train loss:1.031653070943823\n",
      "train loss:0.9502526076691901\n",
      "train loss:0.9232556406227382\n",
      "train loss:0.8527856975544258\n",
      "train loss:0.9533550581711343\n",
      "train loss:0.8806461085857243\n",
      "train loss:0.8837209742247815\n",
      "train loss:0.9132056680998386\n",
      "train loss:0.876493747742795\n",
      "train loss:0.8003972289742529\n",
      "train loss:0.9508725451063889\n",
      "train loss:0.9209004600829266\n",
      "train loss:0.9374190680954082\n",
      "train loss:0.9141407387483979\n",
      "train loss:0.9669023857542794\n",
      "train loss:1.0306759081532615\n",
      "train loss:0.7758665794710663\n",
      "train loss:0.8122263478482364\n",
      "train loss:0.9980664531413087\n",
      "train loss:0.8260634423086951\n",
      "train loss:0.965862316405022\n",
      "train loss:1.0688863458586872\n",
      "train loss:1.0311365316137926\n",
      "train loss:1.0700874219598688\n",
      "train loss:0.8928515701006161\n",
      "train loss:0.9362144398254933\n",
      "train loss:0.8824896320764904\n",
      "train loss:0.9297162416451152\n",
      "train loss:0.853558371433831\n",
      "train loss:0.8675911510364807\n",
      "train loss:0.834347097795558\n",
      "train loss:0.9980982581563547\n",
      "train loss:0.8740258909219603\n",
      "train loss:0.7689632209694244\n",
      "train loss:0.9238719881489827\n",
      "train loss:0.9325512614315277\n",
      "train loss:0.9108343605449432\n",
      "train loss:0.8231648592841345\n",
      "train loss:0.8385612843604098\n",
      "train loss:0.9319845337718501\n",
      "train loss:0.8122859826721206\n",
      "train loss:0.8996396794261247\n",
      "train loss:0.7942133896573327\n",
      "train loss:0.9632836251528418\n",
      "train loss:1.1023095668330272\n",
      "train loss:0.8701154754088486\n",
      "train loss:0.9260268308554602\n",
      "train loss:0.9763629129624364\n",
      "train loss:0.805921164851636\n",
      "train loss:0.9340791342211955\n",
      "train loss:0.9419634329358247\n",
      "train loss:0.8833653479353737\n",
      "train loss:0.9465856637454189\n",
      "train loss:1.0080097825999588\n",
      "train loss:0.8382372387319467\n",
      "train loss:1.0196135377604865\n",
      "train loss:0.818694287227086\n",
      "train loss:1.0242503139998875\n",
      "train loss:0.9778088186059439\n",
      "train loss:0.7739267588900585\n",
      "train loss:1.1049643926656794\n",
      "train loss:1.0129261073022355\n",
      "train loss:0.9148443910067929\n",
      "train loss:0.8766897630308509\n",
      "train loss:1.1163247445659137\n",
      "train loss:0.8528902980683786\n",
      "train loss:0.9935963117475918\n",
      "train loss:0.818461219483534\n",
      "train loss:0.8833273953439751\n",
      "train loss:0.8716804152469667\n",
      "train loss:0.7301778455186024\n",
      "train loss:1.0265587191654475\n",
      "train loss:0.9238607814781578\n",
      "train loss:1.0259782087524674\n",
      "train loss:1.0232112265771653\n",
      "train loss:1.0664096410276107\n",
      "train loss:0.9097372605321478\n",
      "train loss:0.8149786964666685\n",
      "train loss:1.0469899989080775\n",
      "train loss:0.8191644774674494\n",
      "train loss:0.9063471008873928\n",
      "train loss:0.8670736657823034\n",
      "train loss:1.1079683053867209\n",
      "train loss:0.8946700157057498\n",
      "train loss:1.0517927350124987\n",
      "train loss:0.9329269607991997\n",
      "train loss:0.9630836721644329\n",
      "train loss:0.790937055771069\n",
      "train loss:0.9844237582040383\n",
      "train loss:0.9229320408595523\n",
      "train loss:0.9744999157308015\n",
      "train loss:1.0122900635826426\n",
      "train loss:0.8952271101723559\n",
      "train loss:0.8227671838983361\n",
      "train loss:0.9232236531243555\n",
      "train loss:0.8696190098164276\n",
      "train loss:0.8686271001763086\n",
      "train loss:0.9336044337972342\n",
      "train loss:0.8173390269592814\n",
      "train loss:0.9278642875304992\n",
      "train loss:0.8450494436585155\n",
      "train loss:0.8242348228117332\n",
      "train loss:0.8317171200613841\n",
      "train loss:0.8151484728009072\n",
      "train loss:0.9897109152810148\n",
      "train loss:0.7867171249149775\n",
      "train loss:0.7357959740037151\n",
      "train loss:0.8787366199174436\n",
      "train loss:0.8665449390000696\n",
      "train loss:1.0961458309128722\n",
      "train loss:0.8952448197955408\n",
      "train loss:0.9142164387428398\n",
      "train loss:0.8533711904707859\n",
      "train loss:1.0566112909339171\n",
      "train loss:0.923674403576682\n",
      "train loss:0.8628203548951219\n",
      "train loss:0.7607073511736275\n",
      "train loss:0.989751812665618\n",
      "train loss:0.8255573558859068\n",
      "train loss:0.8786006927210078\n",
      "train loss:0.8811552171001885\n",
      "train loss:0.9655672763583777\n",
      "train loss:0.8940116803918337\n",
      "train loss:0.8844880530977506\n",
      "train loss:0.967689087580772\n",
      "train loss:0.9340880095150901\n",
      "train loss:0.952540078350018\n",
      "train loss:0.9756477751332606\n",
      "train loss:0.9196314116738482\n",
      "train loss:0.7553298604446961\n",
      "train loss:0.7969560133470182\n",
      "train loss:0.8855145301588898\n",
      "train loss:0.9088479797781033\n",
      "train loss:0.9638595857662647\n",
      "train loss:0.9795476987825293\n",
      "train loss:1.0714798320706505\n",
      "train loss:0.7620512111447251\n",
      "train loss:0.8672023211340522\n",
      "train loss:0.8954134473263923\n",
      "train loss:1.039852883853942\n",
      "train loss:0.8618833251121716\n",
      "train loss:0.9121346717045578\n",
      "train loss:0.808807207548279\n",
      "train loss:1.015967729646826\n",
      "train loss:0.986046183124419\n",
      "train loss:0.9796557819041182\n",
      "train loss:0.9720343980860843\n",
      "train loss:0.9069309098376145\n",
      "train loss:0.9676028512198046\n",
      "train loss:0.8196652533777755\n",
      "train loss:0.9789081342393819\n",
      "train loss:0.9433420573411756\n",
      "train loss:0.8324227876367961\n",
      "train loss:0.8925688852236171\n",
      "train loss:1.0663634662011408\n",
      "train loss:0.7489949827744584\n",
      "train loss:0.6994119267720285\n",
      "train loss:0.9576145912936291\n",
      "train loss:1.0531840188925907\n",
      "train loss:0.8473961810854058\n",
      "train loss:1.0299180428052166\n",
      "train loss:0.9136398632959154\n",
      "train loss:0.8641756479443526\n",
      "train loss:1.042803670730529\n",
      "train loss:0.899328763132781\n",
      "train loss:0.8801438788821061\n",
      "train loss:0.7920552850521474\n",
      "train loss:0.9968310503058362\n",
      "train loss:0.9404009168598974\n",
      "train loss:0.8685596658593178\n",
      "train loss:0.8863878874045011\n",
      "train loss:0.8792929836391333\n",
      "train loss:0.8615255276741786\n",
      "train loss:0.9519470476343501\n",
      "train loss:0.9345975464783046\n",
      "train loss:0.8714431780548236\n",
      "train loss:0.7234492300065409\n",
      "train loss:0.9393427342495865\n",
      "train loss:0.8921006844951603\n",
      "train loss:0.6890297628069219\n",
      "train loss:0.9544201669601527\n",
      "train loss:0.8691582242602495\n",
      "train loss:0.8814121115231213\n",
      "train loss:0.8077422217740257\n",
      "train loss:0.9981431446634035\n",
      "train loss:0.9966808864017298\n",
      "train loss:0.9670199599912976\n",
      "train loss:0.8989858702439255\n",
      "train loss:1.041428422130787\n",
      "train loss:0.8189712700329842\n",
      "train loss:1.0170973502759626\n",
      "train loss:0.838855365178722\n",
      "train loss:0.8945028265243893\n",
      "train loss:0.80554547799144\n",
      "train loss:0.8475092451183518\n",
      "train loss:0.9179829911010399\n",
      "train loss:1.0218110368584759\n",
      "train loss:0.924526439549218\n",
      "train loss:1.0094880899582541\n",
      "train loss:0.9026136361556675\n",
      "train loss:0.8461628592599282\n",
      "train loss:1.173055468273741\n",
      "train loss:0.8261869446830608\n",
      "train loss:0.9462685780519164\n",
      "train loss:0.9769949347753704\n",
      "train loss:1.0098802570599597\n",
      "train loss:0.8520394859723835\n",
      "train loss:0.9397815082840397\n",
      "=== epoch:5, train acc:0.985, test acc:0.984 ===\n",
      "train loss:0.8669548257232271\n",
      "train loss:0.938187777080627\n",
      "train loss:1.0066294616498974\n",
      "train loss:0.7909981291213006\n",
      "train loss:0.7976772311810653\n",
      "train loss:0.798977852539239\n",
      "train loss:0.9435683938981311\n",
      "train loss:0.9681292103650185\n",
      "train loss:0.8898510801516388\n",
      "train loss:0.863398692799992\n",
      "train loss:0.8825721135577312\n",
      "train loss:0.9580386288781382\n",
      "train loss:0.8590761181699252\n",
      "train loss:0.888113519625431\n",
      "train loss:0.947385143575548\n",
      "train loss:0.8969506717873936\n",
      "train loss:0.8158851738451403\n",
      "train loss:0.8205024045432115\n",
      "train loss:0.9340085078773411\n",
      "train loss:0.9228798293036076\n",
      "train loss:1.0946354420467042\n",
      "train loss:0.8856714975537067\n",
      "train loss:1.098146137650562\n",
      "train loss:0.8761361178337801\n",
      "train loss:0.8114502606047034\n",
      "train loss:0.7811318354126778\n",
      "train loss:0.8969251596755693\n",
      "train loss:0.8696061725653411\n",
      "train loss:0.7464945595521022\n",
      "train loss:0.8616630858107822\n",
      "train loss:0.8093876575504224\n",
      "train loss:0.8324261388802257\n",
      "train loss:0.9891888237088015\n",
      "train loss:0.8265435019135302\n",
      "train loss:0.9875910072665619\n",
      "train loss:0.9365824053619146\n",
      "train loss:0.8199637053021638\n",
      "train loss:0.8470552204988246\n",
      "train loss:0.8875751067300892\n",
      "train loss:0.7403583337230208\n",
      "train loss:0.9214870953807061\n",
      "train loss:1.0052198635305596\n",
      "train loss:1.0688884953932045\n",
      "train loss:0.8935897183919889\n",
      "train loss:0.914477905147107\n",
      "train loss:0.8319980249936612\n",
      "train loss:0.9109173078650081\n",
      "train loss:0.8000860455817169\n",
      "train loss:0.9234179730851011\n",
      "train loss:0.9433267425684027\n",
      "train loss:0.8729013003509913\n",
      "train loss:0.8927548628289841\n",
      "train loss:0.9124774818687926\n",
      "train loss:0.8501372883816428\n",
      "train loss:0.8225725375858682\n",
      "train loss:0.892573995085294\n",
      "train loss:0.8592315591027649\n",
      "train loss:0.9716919098003999\n",
      "train loss:0.8901362963484277\n",
      "train loss:0.9091740954306744\n",
      "train loss:0.9219798847342049\n",
      "train loss:0.9176279243184128\n",
      "train loss:0.9134792767894576\n",
      "train loss:0.8972675430805637\n",
      "train loss:0.8713900129018769\n",
      "train loss:0.9459951754754254\n",
      "train loss:0.8687806762797485\n",
      "train loss:0.9965789458274797\n",
      "train loss:0.9052807162047148\n",
      "train loss:0.9480630567059584\n",
      "train loss:0.9409823860878939\n",
      "train loss:1.0059671031314543\n",
      "train loss:0.8800829924966074\n",
      "train loss:0.720865135298117\n",
      "train loss:1.0006705975334174\n",
      "train loss:0.8602329969917865\n",
      "train loss:0.8556600227587781\n",
      "train loss:0.957243985454169\n",
      "train loss:0.8850604216911446\n",
      "train loss:0.7575388705172066\n",
      "train loss:0.8034042068442045\n",
      "train loss:0.8856072493443662\n",
      "train loss:0.9298292952367511\n",
      "train loss:0.8290940990653758\n",
      "train loss:1.0144201414443894\n",
      "train loss:1.0958729600615102\n",
      "train loss:0.9438613323892389\n",
      "train loss:0.7811204533180716\n",
      "train loss:0.8696098447438488\n",
      "train loss:0.8503593131253543\n",
      "train loss:1.000900095833528\n",
      "train loss:0.9608626978377134\n",
      "train loss:0.9456536164577201\n",
      "train loss:0.8714737288247083\n",
      "train loss:0.9243819174430579\n",
      "train loss:0.9114328232998287\n",
      "train loss:0.9551159000466126\n",
      "train loss:0.9357038212239146\n",
      "train loss:0.9013160802244145\n",
      "train loss:0.9987895742267344\n",
      "train loss:0.853072125608695\n",
      "train loss:0.9768000068646545\n",
      "train loss:1.0122580898135618\n",
      "train loss:0.9720402038851539\n",
      "train loss:0.9048480917906354\n",
      "train loss:0.9511918030593971\n",
      "train loss:0.971868413712607\n",
      "train loss:0.956264431944902\n",
      "train loss:0.8898255892573975\n",
      "train loss:0.916606111983479\n",
      "train loss:1.0425762845509756\n",
      "train loss:0.993863708256002\n",
      "train loss:0.8972168961041601\n",
      "train loss:0.9243685851690515\n",
      "train loss:0.9157925643135326\n",
      "train loss:0.7547894369387944\n",
      "train loss:0.9795141022243153\n",
      "train loss:0.9984255910154196\n",
      "train loss:0.8060178290409045\n",
      "train loss:0.9495001335760392\n",
      "train loss:0.8947772650326747\n",
      "train loss:0.8293623341426231\n",
      "train loss:1.0595257583290378\n",
      "train loss:0.8716707389540518\n",
      "train loss:1.1215166824869893\n",
      "train loss:0.8591863338982344\n",
      "train loss:0.8381534399748277\n",
      "train loss:0.8179720760023586\n",
      "train loss:0.9726015739739118\n",
      "train loss:0.964694569271887\n",
      "train loss:0.7474050999745824\n",
      "train loss:1.0198156153401468\n",
      "train loss:0.7877176012008708\n",
      "train loss:0.9304699410390463\n",
      "train loss:1.012490366109654\n",
      "train loss:1.0044609149420176\n",
      "train loss:0.7730902938016067\n",
      "train loss:0.9191412202339163\n",
      "train loss:0.9285263950888439\n",
      "train loss:0.6885700725786073\n",
      "train loss:0.872357822769172\n",
      "train loss:1.0676733207251408\n",
      "train loss:0.981176474173982\n",
      "train loss:1.0358817489691885\n",
      "train loss:0.9260512157764127\n",
      "train loss:0.8824082289817042\n",
      "train loss:0.9723227742110818\n",
      "train loss:0.8247958745411688\n",
      "train loss:0.8705126378350133\n",
      "train loss:0.8258143956876257\n",
      "train loss:0.8513708016077145\n",
      "train loss:0.9590606778818347\n",
      "train loss:0.8902477994333671\n",
      "train loss:1.014724478295495\n",
      "train loss:0.846570319376514\n",
      "train loss:0.9489479932296778\n",
      "train loss:0.9273255621583282\n",
      "train loss:0.9823047409910692\n",
      "train loss:0.9594664250220489\n",
      "train loss:0.8751245279514264\n",
      "train loss:0.7590282261558643\n",
      "train loss:1.0478335377717636\n",
      "train loss:1.0023568498409339\n",
      "train loss:1.045200136394038\n",
      "train loss:1.0788263152215816\n",
      "train loss:0.8813185094681951\n",
      "train loss:0.914590561290444\n",
      "train loss:0.8506888609885687\n",
      "train loss:0.8867113341774819\n",
      "train loss:0.9349920482085521\n",
      "train loss:0.8974414430385551\n",
      "train loss:0.9523802390500405\n",
      "train loss:0.8640748810458987\n",
      "train loss:0.9220448227542696\n",
      "train loss:0.8700228708692189\n",
      "train loss:0.6851490254742088\n",
      "train loss:0.9524306026349203\n",
      "train loss:0.8124736042099543\n",
      "train loss:1.0095201043477096\n",
      "train loss:0.8130551458636252\n",
      "train loss:0.794986661261064\n",
      "train loss:0.9738165760671886\n",
      "train loss:0.7915546126549559\n",
      "train loss:1.0287446278050982\n",
      "train loss:1.0418382362356704\n",
      "train loss:0.7449201271714954\n",
      "train loss:0.9221819089160291\n",
      "train loss:0.9939499886298018\n",
      "train loss:0.8999869016065806\n",
      "train loss:1.0060031622970576\n",
      "train loss:0.7881707035492039\n",
      "train loss:0.8780645051496221\n",
      "train loss:0.8801401045105\n",
      "train loss:0.9239461374814715\n",
      "train loss:1.0008630794242617\n",
      "train loss:0.8806054549264866\n",
      "train loss:0.7991985046776382\n",
      "train loss:0.9612776356560616\n",
      "train loss:0.9666950380300365\n",
      "train loss:0.9410213040917458\n",
      "train loss:0.9207717922418648\n",
      "train loss:0.9953121851895084\n",
      "train loss:1.1091451268581205\n",
      "train loss:0.8542199312795411\n",
      "train loss:0.7407115010127131\n",
      "train loss:0.941297677522147\n",
      "train loss:0.8705260610333437\n",
      "train loss:0.8801147873526406\n",
      "train loss:0.9600928552775116\n",
      "train loss:1.0675186196710182\n",
      "train loss:0.8860517810515498\n",
      "train loss:0.740564728692945\n",
      "train loss:0.8309842098891451\n",
      "train loss:0.8579248740483453\n",
      "train loss:0.8515495078071694\n",
      "train loss:0.8771753125569767\n",
      "train loss:1.0029597183146834\n",
      "train loss:0.9776824716370035\n",
      "train loss:1.0803475516010181\n",
      "train loss:1.0820226082536726\n",
      "train loss:0.8620251024529039\n",
      "train loss:1.0136440106491924\n",
      "train loss:0.8617415285087983\n",
      "train loss:0.9216885731026138\n",
      "train loss:0.8469088347669779\n",
      "train loss:1.029665267950017\n",
      "train loss:0.9783339049010241\n",
      "train loss:0.7737269305616428\n",
      "train loss:0.8021320863325242\n",
      "train loss:0.9720779218008783\n",
      "train loss:0.6817010369113587\n",
      "train loss:1.0179586354773642\n",
      "train loss:0.9352518558235821\n",
      "train loss:0.9237184984844776\n",
      "train loss:1.075526236109\n",
      "train loss:0.9878775822038633\n",
      "train loss:0.6268719883702084\n",
      "train loss:0.790577751704779\n",
      "train loss:0.8832928014471466\n",
      "train loss:1.1012252393013184\n",
      "train loss:0.8005519221186844\n",
      "train loss:0.9629214465294598\n",
      "train loss:0.9023858400118785\n",
      "train loss:0.9875268710752075\n",
      "train loss:0.8515475472885343\n",
      "train loss:1.0991338950154317\n",
      "train loss:0.9807235781769928\n",
      "train loss:0.7284601479829205\n",
      "train loss:0.9713487186180533\n",
      "train loss:0.9740157320129065\n",
      "train loss:0.8379262922860684\n",
      "train loss:0.9336910733306832\n",
      "train loss:0.8345787382459807\n",
      "train loss:0.8663557138657471\n",
      "train loss:1.0821327538465546\n",
      "train loss:1.0102953291274155\n",
      "train loss:0.9986409251775729\n",
      "train loss:0.9625060768734809\n",
      "train loss:1.0052801459143916\n",
      "train loss:1.0843057257837072\n",
      "train loss:0.8989068447943894\n",
      "train loss:0.8038999673733079\n",
      "train loss:0.947131967558127\n",
      "train loss:0.8948030859685012\n",
      "train loss:0.7716144150805263\n",
      "train loss:0.853255878932922\n",
      "train loss:0.7272997541516893\n",
      "train loss:0.9695344950734952\n",
      "train loss:0.8945519591049103\n",
      "train loss:0.9676622179563991\n",
      "train loss:0.9486904276470425\n",
      "train loss:0.8975820371808395\n",
      "train loss:0.8726741016197372\n",
      "train loss:0.6500966803562414\n",
      "train loss:1.0471816851223508\n",
      "train loss:0.939466854634295\n",
      "train loss:0.8646404714358501\n",
      "train loss:1.0276427451290748\n",
      "train loss:0.7735187286567357\n",
      "train loss:0.9011387889657854\n",
      "train loss:0.9201147900301805\n",
      "train loss:0.8750963883487064\n",
      "train loss:0.9895645604831219\n",
      "train loss:0.8767117530002925\n",
      "train loss:0.6176307154677244\n",
      "train loss:0.9802505795592984\n",
      "train loss:0.9173783735331524\n",
      "train loss:0.9361054014480729\n",
      "train loss:0.8922394668863083\n",
      "train loss:1.0511789580639306\n",
      "train loss:0.9148146297407961\n",
      "train loss:1.0981954445623447\n",
      "train loss:0.905863077222172\n",
      "train loss:0.7499938342573097\n",
      "train loss:0.9151558237124928\n",
      "train loss:0.7909403231296172\n",
      "train loss:0.6396985556962863\n",
      "train loss:0.8718703769282321\n",
      "train loss:0.836521203257735\n",
      "train loss:0.7645047132568062\n",
      "train loss:0.9768662113369715\n",
      "train loss:0.7679181808313461\n",
      "train loss:0.920596564662836\n",
      "train loss:0.7399471796038952\n",
      "train loss:0.9534449816656341\n",
      "train loss:0.7378047482263361\n",
      "train loss:0.9945772357992747\n",
      "train loss:0.9504062353267357\n",
      "train loss:0.8836073452375893\n",
      "train loss:0.625695388200226\n",
      "train loss:0.9388822249386684\n",
      "train loss:0.861246629917967\n",
      "train loss:0.9159251639490107\n",
      "train loss:1.0210520866016035\n",
      "train loss:0.8939358702187938\n",
      "train loss:0.8693448519719402\n",
      "train loss:0.8228523021944616\n",
      "train loss:0.7671176861862296\n",
      "train loss:0.8488196682005354\n",
      "train loss:0.9787874454579911\n",
      "train loss:0.9961417049580334\n",
      "train loss:0.6807434965967698\n",
      "train loss:0.8658385555116908\n",
      "train loss:0.9919577185458691\n",
      "train loss:0.9109591481067024\n",
      "train loss:0.7679295402978269\n",
      "train loss:0.9064255275528691\n",
      "train loss:0.9287538947970826\n",
      "train loss:0.983083138501445\n",
      "train loss:1.005689696496424\n",
      "train loss:0.8381619089002966\n",
      "train loss:0.9647796392654604\n",
      "train loss:0.9532320087880631\n",
      "train loss:0.8494002660910414\n",
      "train loss:0.9755918564413983\n",
      "train loss:0.933528707953539\n",
      "train loss:0.8739663494143819\n",
      "train loss:0.7790863866638573\n",
      "train loss:1.0391699364677753\n",
      "train loss:0.8160632360109051\n",
      "train loss:0.9694532968836637\n",
      "train loss:0.8419616367052879\n",
      "train loss:0.8663010001792933\n",
      "train loss:0.8704728784124514\n",
      "train loss:0.8067675888695327\n",
      "train loss:0.7784453894248371\n",
      "train loss:1.0149972110763117\n",
      "train loss:0.88400043900022\n",
      "train loss:0.8856302062297776\n",
      "train loss:0.954698304480756\n",
      "train loss:0.7583704036708167\n",
      "train loss:0.9133314538671852\n",
      "train loss:0.8516977693226626\n",
      "train loss:1.0575021316141595\n",
      "train loss:0.9310872301772513\n",
      "train loss:0.8930401343532296\n",
      "train loss:0.9634517354285557\n",
      "train loss:0.8908574659582833\n",
      "train loss:0.9084138726379477\n",
      "train loss:0.9077125519524905\n",
      "train loss:0.9790155316765826\n",
      "train loss:0.8744917850526689\n",
      "train loss:0.9738990011580333\n",
      "train loss:1.0043071356881896\n",
      "train loss:0.9705473348983652\n",
      "train loss:1.0749496153390545\n",
      "train loss:0.9124139973547316\n",
      "train loss:0.9979410819204857\n",
      "train loss:0.8612622821558401\n",
      "train loss:0.8772495524732995\n",
      "train loss:0.8454571709056558\n",
      "train loss:1.0677642297595844\n",
      "train loss:1.1135532715822503\n",
      "train loss:0.9921003649416126\n",
      "train loss:0.8657306289452241\n",
      "train loss:1.0042307849992615\n",
      "train loss:1.0908409962687386\n",
      "train loss:1.066440818118759\n",
      "train loss:0.9591637984517258\n",
      "train loss:0.9092856962188509\n",
      "train loss:0.7694249752135986\n",
      "train loss:0.8374064910031579\n",
      "train loss:0.8563477510263613\n",
      "train loss:0.684727562119683\n",
      "train loss:0.8913673656372042\n",
      "train loss:0.8034472242742646\n",
      "train loss:0.8892021582932931\n",
      "train loss:0.9247214559043921\n",
      "train loss:0.9380952999521155\n",
      "train loss:0.8433489086165709\n",
      "train loss:0.9112318193787533\n",
      "train loss:1.0203441084875782\n",
      "train loss:0.949945966983996\n",
      "train loss:0.8972418467801397\n",
      "train loss:0.8018193317290028\n",
      "train loss:1.1349132867690854\n",
      "train loss:0.8450542419316284\n",
      "train loss:0.8629261531670893\n",
      "train loss:0.8937976969384384\n",
      "train loss:0.8047820472255788\n",
      "train loss:0.8337786922128483\n",
      "train loss:0.926187843252601\n",
      "train loss:0.7972194297709752\n",
      "train loss:0.8889578790112189\n",
      "train loss:0.7214300078055034\n",
      "train loss:0.9194810459463452\n",
      "train loss:0.8343586094491843\n",
      "train loss:0.880450742760476\n",
      "train loss:1.00630234529697\n",
      "train loss:0.9558770082295976\n",
      "train loss:0.8205546985291916\n",
      "train loss:0.8944984855766074\n",
      "train loss:0.804506207744404\n",
      "train loss:0.86533699136472\n",
      "train loss:0.9465950027815895\n",
      "train loss:0.9978462322182771\n",
      "train loss:0.958483535250226\n",
      "train loss:0.8826268464903987\n",
      "train loss:0.9651662872919985\n",
      "train loss:0.879467958705448\n",
      "train loss:1.0780989811890067\n",
      "train loss:1.0197278144711053\n",
      "train loss:0.9095900381708663\n",
      "train loss:0.8983717896895197\n",
      "train loss:1.007036640748778\n",
      "train loss:0.7929302812506077\n",
      "train loss:0.9845712321116876\n",
      "train loss:0.8279277689871235\n",
      "train loss:0.8979105259274606\n",
      "train loss:0.7506215380862389\n",
      "train loss:0.7893363126397942\n",
      "train loss:0.7710713308535111\n",
      "train loss:0.9370867285874361\n",
      "train loss:0.8737749683630015\n",
      "train loss:0.8042245629939315\n",
      "train loss:0.9390625703986192\n",
      "train loss:0.9705571281575628\n",
      "train loss:1.0253210603788028\n",
      "train loss:0.9098101835307518\n",
      "train loss:0.8324523917569185\n",
      "train loss:0.9613703900544585\n",
      "train loss:0.9200106223303048\n",
      "train loss:0.8238991549862044\n",
      "train loss:0.9346352617175531\n",
      "train loss:0.910663115702579\n",
      "train loss:0.7544091002803215\n",
      "train loss:0.8869335381215966\n",
      "train loss:0.8212392206327661\n",
      "train loss:0.9464205045370305\n",
      "train loss:1.0631480066621284\n",
      "train loss:0.8982436879872161\n",
      "train loss:1.0757260888112667\n",
      "train loss:0.9628194985143742\n",
      "train loss:1.04029644879761\n",
      "train loss:1.0519781439087204\n",
      "train loss:0.8903140691231672\n",
      "train loss:0.9187944956531809\n",
      "train loss:0.9751630017554674\n",
      "train loss:0.8390213741657978\n",
      "train loss:0.801156447660174\n",
      "train loss:0.8214719521045866\n",
      "train loss:0.8981112165098192\n",
      "train loss:0.7614601817076168\n",
      "train loss:0.9129280314457477\n",
      "train loss:1.0069690421392272\n",
      "train loss:0.9049446369435664\n",
      "train loss:0.9146905013507431\n",
      "train loss:0.8644527433075655\n",
      "train loss:0.8832639934664782\n",
      "train loss:1.0833524441202345\n",
      "train loss:0.7489300300790767\n",
      "train loss:0.8578503705604512\n",
      "train loss:0.7394786012497248\n",
      "train loss:0.8438029404149693\n",
      "train loss:0.784790783804874\n",
      "train loss:0.9850954470394632\n",
      "train loss:0.8275607869768179\n",
      "train loss:0.9607454052566379\n",
      "train loss:0.8963576171252504\n",
      "train loss:0.8463942855869316\n",
      "train loss:0.9460160992928531\n",
      "train loss:1.0356442155699903\n",
      "train loss:0.8139751704572429\n",
      "train loss:0.8167880093231813\n",
      "train loss:0.9546670316667943\n",
      "train loss:0.9109890543994542\n",
      "train loss:0.926396404580792\n",
      "train loss:0.9397341256542944\n",
      "train loss:1.0094822887455737\n",
      "train loss:0.9515894896318383\n",
      "train loss:1.0118214864301185\n",
      "train loss:0.9154807369226972\n",
      "train loss:1.028745177416427\n",
      "train loss:0.8739855826855204\n",
      "train loss:0.8533140944925591\n",
      "train loss:0.9458396492693391\n",
      "train loss:0.9024907331779107\n",
      "train loss:0.9205265980810522\n",
      "train loss:0.9677587311779741\n",
      "train loss:0.9012524531440232\n",
      "train loss:0.8881159267506884\n",
      "train loss:0.9132898956763401\n",
      "train loss:0.7733499989744851\n",
      "train loss:0.9090565730871351\n",
      "train loss:0.9211088922027061\n",
      "train loss:1.0950562543946143\n",
      "train loss:0.823923917424696\n",
      "train loss:0.9050903223998923\n",
      "train loss:1.0189412319641646\n",
      "train loss:1.0069149425323771\n",
      "train loss:0.7323736195406997\n",
      "train loss:0.8602491368920478\n",
      "train loss:0.9262525360809035\n",
      "train loss:1.0803562140649243\n",
      "train loss:0.9405217913350278\n",
      "train loss:1.0070090201915174\n",
      "train loss:0.8812490607435506\n",
      "train loss:1.0276442229412917\n",
      "train loss:0.6465322426749555\n",
      "train loss:0.9048963495860207\n",
      "train loss:0.8829899461682111\n",
      "train loss:0.9809202419973418\n",
      "train loss:0.8673685595464549\n",
      "train loss:0.8622677858796396\n",
      "train loss:0.907506747211196\n",
      "train loss:1.075977053047933\n",
      "train loss:0.8292011434886986\n",
      "train loss:0.9296494673094631\n",
      "train loss:0.8376269475873624\n",
      "train loss:0.8443469590448275\n",
      "train loss:0.8244729668077595\n",
      "train loss:1.0084341871052274\n",
      "train loss:0.8431996634502688\n",
      "train loss:0.8939772837829356\n",
      "train loss:1.0031130187159898\n",
      "train loss:0.9478201954452985\n",
      "train loss:0.8902011240488696\n",
      "train loss:1.0317901844400161\n",
      "train loss:0.8330676196664142\n",
      "train loss:0.9806127651007311\n",
      "train loss:0.8780951119120144\n",
      "train loss:0.8782562860537159\n",
      "train loss:1.12753490156567\n",
      "train loss:0.7817156061350363\n",
      "train loss:0.8158636260618265\n",
      "train loss:0.6469615953117673\n",
      "train loss:1.010590953920986\n",
      "train loss:0.9144827176731302\n",
      "train loss:0.7652093834868607\n",
      "train loss:0.7882180722041106\n",
      "train loss:0.8827031808392717\n",
      "train loss:0.8520632049239303\n",
      "train loss:0.944922478484\n",
      "train loss:0.8051930589605932\n",
      "train loss:0.9099057040067889\n",
      "train loss:0.8675658097206189\n",
      "train loss:0.7843740467357775\n",
      "train loss:0.8170073207731032\n",
      "train loss:0.8880988468257317\n",
      "train loss:0.8691206945213176\n",
      "train loss:0.9031930983919836\n",
      "train loss:0.9465107948957487\n",
      "train loss:0.9693049752126849\n",
      "train loss:0.8925795234764639\n",
      "train loss:0.9315814164954266\n",
      "train loss:0.8321227042143333\n",
      "train loss:0.7730607214944087\n",
      "train loss:0.929819970594716\n",
      "train loss:0.8703364331078544\n",
      "train loss:1.034446664872819\n",
      "train loss:1.1031824017108578\n",
      "train loss:0.9441299619355736\n",
      "train loss:1.0050876208600705\n",
      "train loss:1.0604860010206159\n",
      "train loss:0.9526732778995252\n",
      "train loss:1.01620507369135\n",
      "train loss:0.958429015583325\n",
      "train loss:0.9327272632382635\n",
      "train loss:0.7963331011514982\n",
      "train loss:0.8978429032733771\n",
      "train loss:0.8713866806592644\n",
      "train loss:1.0177832392586814\n",
      "train loss:0.9895471907842963\n",
      "train loss:0.8646383149105855\n",
      "train loss:0.7972024121257032\n",
      "train loss:0.8647324051861244\n",
      "train loss:0.8802568276125052\n",
      "train loss:1.0684080140390548\n",
      "train loss:0.7528590668959236\n",
      "train loss:0.8685117248550869\n",
      "train loss:0.8664656634855988\n",
      "train loss:0.7049956651847167\n",
      "train loss:0.7998868557293694\n",
      "train loss:1.0358805497698114\n",
      "train loss:1.0780247469327449\n",
      "train loss:1.0455456297856163\n",
      "train loss:0.9155622337187946\n",
      "train loss:0.88951864440308\n",
      "train loss:0.782486435814196\n",
      "train loss:0.8846709263515349\n",
      "=== epoch:6, train acc:0.989, test acc:0.989 ===\n",
      "train loss:0.981693819299169\n",
      "train loss:1.0327712990607507\n",
      "train loss:0.8303357949773167\n",
      "train loss:0.9480261927920524\n",
      "train loss:0.8757461135551821\n",
      "train loss:0.8085223393527053\n",
      "train loss:0.9156981240314132\n",
      "train loss:0.8473706880799226\n",
      "train loss:0.9545758767687178\n",
      "train loss:0.8783658430380089\n",
      "train loss:1.00848403071336\n",
      "train loss:0.8891403192129239\n",
      "train loss:0.9280916409037249\n",
      "train loss:1.010217002995455\n",
      "train loss:1.048499870556503\n",
      "train loss:0.9501954348756257\n",
      "train loss:0.9872042577032736\n",
      "train loss:0.8168145520880198\n",
      "train loss:0.9884506150757519\n",
      "train loss:0.7318693372581468\n",
      "train loss:0.9011881255407224\n",
      "train loss:0.809856428259137\n",
      "train loss:0.8557610303999038\n",
      "train loss:0.8707904213255777\n",
      "train loss:0.7505251654112248\n",
      "train loss:0.7573723517816869\n",
      "train loss:0.8790203613358315\n",
      "train loss:0.7159245851099497\n",
      "train loss:0.9467344800153167\n",
      "train loss:0.9171684035942097\n",
      "train loss:0.9679403298906357\n",
      "train loss:0.9087341429456609\n",
      "train loss:0.8920193481973208\n",
      "train loss:0.9113575124684848\n",
      "train loss:0.8785356234124371\n",
      "train loss:0.8949794027202903\n",
      "train loss:0.959029359849235\n",
      "train loss:0.9074416698536871\n",
      "train loss:0.9070557477402056\n",
      "train loss:0.8212124610358877\n",
      "train loss:0.8701503742983716\n",
      "train loss:0.9250772254741206\n",
      "train loss:0.8911482044825072\n",
      "train loss:0.8022757607633778\n",
      "train loss:1.052109428681025\n",
      "train loss:0.956826472567247\n",
      "train loss:0.8767904135312418\n",
      "train loss:1.0261321656397646\n",
      "train loss:0.8659729073058159\n",
      "train loss:0.8846851952181518\n",
      "train loss:1.0726126421762747\n",
      "train loss:0.8890018197299793\n",
      "train loss:0.8148218607343142\n",
      "train loss:1.023968204916122\n",
      "train loss:0.9109095731451076\n",
      "train loss:0.826352355009586\n",
      "train loss:0.6839756726466724\n",
      "train loss:0.707512881888274\n",
      "train loss:0.8990091635211797\n",
      "train loss:0.8531046677210987\n",
      "train loss:0.904184562970378\n",
      "train loss:0.7859819512544854\n",
      "train loss:0.605605939929344\n",
      "train loss:1.0507394261267546\n",
      "train loss:0.9190443081581121\n",
      "train loss:1.0506698231776843\n",
      "train loss:0.9363252642924672\n",
      "train loss:0.8636126335006697\n",
      "train loss:0.8829264900187159\n",
      "train loss:0.9380463702208379\n",
      "train loss:0.9382345227698131\n",
      "train loss:1.039288969326581\n",
      "train loss:0.8530599683007651\n",
      "train loss:0.8006584016386762\n",
      "train loss:1.062280604864164\n",
      "train loss:0.7066333665147458\n",
      "train loss:0.875177904548647\n",
      "train loss:0.9359755190836933\n",
      "train loss:0.9633203143696342\n",
      "train loss:1.0435300616248124\n",
      "train loss:0.9901089262514462\n",
      "train loss:0.8427417840809366\n",
      "train loss:0.9166499960303471\n",
      "train loss:1.1563208812901282\n",
      "train loss:0.9704779755421827\n",
      "train loss:0.8982423088843683\n",
      "train loss:0.9777776642236913\n",
      "train loss:0.975554917920015\n",
      "train loss:0.9807472298783106\n",
      "train loss:0.9712595080809013\n",
      "train loss:0.8161426772183193\n",
      "train loss:1.0625458745342058\n",
      "train loss:0.9667479144405307\n",
      "train loss:0.9605375153956542\n",
      "train loss:0.6551727653543202\n",
      "train loss:0.8567912248423981\n",
      "train loss:0.9438736651552502\n",
      "train loss:1.0727671079464594\n",
      "train loss:0.8833873304968616\n",
      "train loss:0.8722149701242617\n",
      "train loss:1.040066617006541\n",
      "train loss:0.9094319615907313\n",
      "train loss:0.9102928023399605\n",
      "train loss:0.9352304089020282\n",
      "train loss:0.9760703020454368\n",
      "train loss:0.8557960073606836\n",
      "train loss:0.9502798115810485\n",
      "train loss:0.95992936290069\n",
      "train loss:0.9791897299641069\n",
      "train loss:0.9082143654195565\n",
      "train loss:0.9508861742680635\n",
      "train loss:0.9465680041678796\n",
      "train loss:0.7681569331768688\n",
      "train loss:0.9570168086796379\n",
      "train loss:0.9543357565437779\n",
      "train loss:0.9226474139944536\n",
      "train loss:0.9979137887194376\n",
      "train loss:0.9580851253269735\n",
      "train loss:0.8746315142022871\n",
      "train loss:0.8878152226786561\n",
      "train loss:1.0356091186418865\n",
      "train loss:0.830464331052922\n",
      "train loss:0.6819222125487491\n",
      "train loss:0.7214603767288271\n",
      "train loss:0.9544908674025475\n",
      "train loss:0.9661368046542884\n",
      "train loss:0.8325250984924059\n",
      "train loss:0.8562925275523026\n",
      "train loss:1.0313454317979402\n",
      "train loss:0.8815271316885297\n",
      "train loss:0.819311856713265\n",
      "train loss:0.8960410597139944\n",
      "train loss:0.9833075127088522\n",
      "train loss:0.8565842674324473\n",
      "train loss:0.9049260702224882\n",
      "train loss:0.9071158460818074\n",
      "train loss:1.1085569026778246\n",
      "train loss:0.8870922271907483\n",
      "train loss:0.7099388906665909\n",
      "train loss:0.7936782135349756\n",
      "train loss:0.8080030061534128\n",
      "train loss:1.0520065003026027\n",
      "train loss:0.85049515084926\n",
      "train loss:0.8798481089483974\n",
      "train loss:1.0435524724645546\n",
      "train loss:0.9393894302851785\n",
      "train loss:0.8406188058366939\n",
      "train loss:0.9807456690350017\n",
      "train loss:0.9693748629092318\n",
      "train loss:1.0629000866296483\n",
      "train loss:0.9917558208759556\n",
      "train loss:0.7892774168170067\n",
      "train loss:0.8602904737612209\n",
      "train loss:0.9141872591703115\n",
      "train loss:0.949091803370725\n",
      "train loss:0.665788433336658\n",
      "train loss:0.9788337113918364\n",
      "train loss:0.8175102530857723\n",
      "train loss:0.789163394997361\n",
      "train loss:0.8627790785529464\n",
      "train loss:0.9322275657555454\n",
      "train loss:0.7940526234765671\n",
      "train loss:0.9158452842453211\n",
      "train loss:0.8851311060427939\n",
      "train loss:0.9469608387994173\n",
      "train loss:0.8352648703961785\n",
      "train loss:0.9612326621434027\n",
      "train loss:0.9248364546892787\n",
      "train loss:1.1049806911443123\n",
      "train loss:0.7883626901975315\n",
      "train loss:0.8561574528753672\n",
      "train loss:0.7035672803296987\n",
      "train loss:1.1110818323416936\n",
      "train loss:0.9091808413463767\n",
      "train loss:0.9296858756222299\n",
      "train loss:0.8227415732894633\n",
      "train loss:0.9381277195815511\n",
      "train loss:0.977109465189888\n",
      "train loss:0.9061091225510364\n",
      "train loss:0.8379880070766329\n",
      "train loss:0.9560899594426531\n",
      "train loss:0.7936531626594074\n",
      "train loss:0.861576697979777\n",
      "train loss:0.9392557801155372\n",
      "train loss:0.8914778014628086\n",
      "train loss:0.9030182208312203\n",
      "train loss:0.8058109772388973\n",
      "train loss:0.8115386686758598\n",
      "train loss:1.008959805598536\n",
      "train loss:0.9441015176484069\n",
      "train loss:0.9561787191037406\n",
      "train loss:0.9296046270437642\n",
      "train loss:0.8802810821977454\n",
      "train loss:0.8251038207455822\n",
      "train loss:0.9078621703426474\n",
      "train loss:1.0856513288143739\n",
      "train loss:0.8207364797472703\n",
      "train loss:0.7723449211461305\n",
      "train loss:0.8527298405091424\n",
      "train loss:0.8471141257918273\n",
      "train loss:0.8795564915225899\n",
      "train loss:0.9797646024862188\n",
      "train loss:0.6128194669564679\n",
      "train loss:0.8560980327313973\n",
      "train loss:0.905186092822714\n",
      "train loss:0.8483125449282686\n",
      "train loss:0.8063254411474869\n",
      "train loss:0.7982615780094008\n",
      "train loss:0.9117890660092313\n",
      "train loss:0.7660665119632264\n",
      "train loss:0.7681552943692711\n",
      "train loss:1.0334522487393543\n",
      "train loss:1.08375832005395\n",
      "train loss:0.8736251193486565\n",
      "train loss:0.9205408661294954\n",
      "train loss:0.8002662559071377\n",
      "train loss:0.8345150468323302\n",
      "train loss:0.9968540963722162\n",
      "train loss:0.9536441387836667\n",
      "train loss:0.8653140940236911\n",
      "train loss:0.7257017634406975\n",
      "train loss:1.0162144270310953\n",
      "train loss:0.9685821299265625\n",
      "train loss:0.8447970594556716\n",
      "train loss:0.8640858228255632\n",
      "train loss:0.9132304488325019\n",
      "train loss:1.001040579610813\n",
      "train loss:0.9297734169956036\n",
      "train loss:1.0317658514643964\n",
      "train loss:0.8246161092001758\n",
      "train loss:0.9991339643908529\n",
      "train loss:0.9252976392651451\n",
      "train loss:0.8440730274790268\n",
      "train loss:1.1156640612441842\n",
      "train loss:1.0185599572861639\n",
      "train loss:0.8776667015826597\n",
      "train loss:0.9902482251787454\n",
      "train loss:0.9863171496669403\n",
      "train loss:1.1242293944148452\n",
      "train loss:0.7936832842506324\n",
      "train loss:0.970678483794646\n",
      "train loss:0.9904299361017411\n",
      "train loss:0.8979036210158001\n",
      "train loss:0.8655810699269593\n",
      "train loss:0.794072855719695\n",
      "train loss:0.8447175490039257\n",
      "train loss:0.8806893925291636\n",
      "train loss:0.7965921987578817\n",
      "train loss:0.7265324338795175\n",
      "train loss:0.8813143591195857\n",
      "train loss:1.001651249847342\n",
      "train loss:1.0201848665389353\n",
      "train loss:0.9084592103060974\n",
      "train loss:0.8886985603511033\n",
      "train loss:0.8864160712924644\n",
      "train loss:0.9422882737703031\n",
      "train loss:0.8469351360233435\n",
      "train loss:0.9039456321647353\n",
      "train loss:0.8837234675175607\n",
      "train loss:0.9617876054157785\n",
      "train loss:0.8396292331683051\n",
      "train loss:0.9793230037051178\n",
      "train loss:0.9356199409931182\n",
      "train loss:0.9079906275020555\n",
      "train loss:0.8954447491444384\n",
      "train loss:0.9269387324213975\n",
      "train loss:0.9177358029695596\n",
      "train loss:0.9949525479585758\n",
      "train loss:1.1034626251428064\n",
      "train loss:0.8713836168626585\n",
      "train loss:0.796410915132234\n",
      "train loss:0.9444573102654462\n",
      "train loss:0.8260249255230038\n",
      "train loss:0.7733730830401797\n",
      "train loss:0.9672607246620609\n",
      "train loss:1.0321125144301364\n",
      "train loss:0.9024153193400096\n",
      "train loss:0.9508035187980914\n",
      "train loss:0.9234966983261077\n",
      "train loss:0.8286111188985688\n",
      "train loss:0.7850692317621579\n",
      "train loss:0.8715635934177008\n",
      "train loss:1.0089114563082615\n",
      "train loss:0.8397534979174958\n",
      "train loss:0.8432689327971578\n",
      "train loss:0.9182092890232803\n",
      "train loss:0.9264041679896792\n",
      "train loss:0.7979723813978152\n",
      "train loss:0.7219713793290552\n",
      "train loss:0.7713686902063133\n",
      "train loss:0.950650569815529\n",
      "train loss:1.0109810247978561\n",
      "train loss:0.8321554993118128\n",
      "train loss:0.872969516324641\n",
      "train loss:0.8534188494190188\n",
      "train loss:0.978768326403681\n",
      "train loss:0.8790560348274492\n",
      "train loss:0.8037382905114103\n",
      "train loss:0.7886988708461586\n",
      "train loss:0.9089463860379559\n",
      "train loss:0.8824941823387031\n",
      "train loss:0.7987189142734992\n",
      "train loss:0.9718302388502988\n",
      "train loss:0.9024081288214209\n",
      "train loss:0.9875362698562252\n",
      "train loss:0.8617002075823312\n",
      "train loss:0.8006135522056277\n",
      "train loss:0.7365293179409871\n",
      "train loss:0.81264275703634\n",
      "train loss:1.0347231565882278\n",
      "train loss:0.913722453350814\n",
      "train loss:1.1694021468050164\n",
      "train loss:0.8706438332335815\n",
      "train loss:0.8768713030935926\n",
      "train loss:0.9059906899114344\n",
      "train loss:0.7933959357012992\n",
      "train loss:0.9832610892646724\n",
      "train loss:0.8280805736801381\n",
      "train loss:0.811685083845898\n",
      "train loss:0.9512181818114079\n",
      "train loss:0.9129835744569115\n",
      "train loss:0.9222024943051234\n",
      "train loss:0.8867157200422446\n",
      "train loss:0.7570846345433355\n",
      "train loss:0.9766206343258361\n",
      "train loss:0.9955299820059144\n",
      "train loss:0.9851448323250449\n",
      "train loss:0.8906315689688123\n",
      "train loss:0.7600055287262429\n",
      "train loss:0.7651432058182068\n",
      "train loss:0.8051095522585037\n",
      "train loss:1.1658030489773021\n",
      "train loss:0.9595949254250601\n",
      "train loss:0.9949222049841075\n",
      "train loss:0.7419868240117286\n",
      "train loss:0.9148570996193145\n",
      "train loss:0.9929392221734505\n",
      "train loss:0.9093954564864509\n",
      "train loss:0.8506761679089766\n",
      "train loss:1.0587416746022518\n",
      "train loss:0.8958418364618325\n",
      "train loss:0.7915284147645505\n",
      "train loss:1.0767736106347237\n",
      "train loss:0.8889580072756886\n",
      "train loss:0.7382709896272849\n",
      "train loss:0.9725431691967557\n",
      "train loss:0.8449053691291133\n",
      "train loss:1.0504612388558576\n",
      "train loss:0.8912690944027459\n",
      "train loss:0.8562178190669026\n",
      "train loss:0.9151600996827388\n",
      "train loss:0.9519245890935278\n",
      "train loss:0.7882999300898577\n",
      "train loss:0.9159190253215107\n",
      "train loss:1.032700745205887\n",
      "train loss:0.9556102796452316\n",
      "train loss:1.0444722162548892\n",
      "train loss:0.8784126164552579\n",
      "train loss:0.7836836975732391\n",
      "train loss:0.9216221697211777\n",
      "train loss:0.7846347871395564\n",
      "train loss:1.0642645270092652\n",
      "train loss:0.9988255997720259\n",
      "train loss:0.839299355814796\n",
      "train loss:0.8794729402540403\n",
      "train loss:0.9007100215619352\n",
      "train loss:1.0330278851722345\n",
      "train loss:0.8478745885080389\n",
      "train loss:0.9911603402051908\n",
      "train loss:0.8266933185600946\n",
      "train loss:0.7496765592397563\n",
      "train loss:0.9866664920872574\n",
      "train loss:1.0701919766999881\n",
      "train loss:0.9311199366038448\n",
      "train loss:0.7894499347019\n",
      "train loss:0.84866636846771\n",
      "train loss:0.7986794698431461\n",
      "train loss:0.8812275767301313\n",
      "train loss:0.8991249401480849\n",
      "train loss:0.8506317606902872\n",
      "train loss:0.9812193618208954\n",
      "train loss:0.9199178382969287\n",
      "train loss:0.8224758600048862\n",
      "train loss:0.9124799568447645\n",
      "train loss:0.819381692530453\n",
      "train loss:0.9129337649051089\n",
      "train loss:0.956262325517007\n",
      "train loss:0.891048841519575\n",
      "train loss:0.9035451026739193\n",
      "train loss:1.1094624352472953\n",
      "train loss:0.9521590374065668\n",
      "train loss:0.8417540061312512\n",
      "train loss:0.744316300802503\n",
      "train loss:0.8276312794857057\n",
      "train loss:0.9222108604133562\n",
      "train loss:1.0225513520299092\n",
      "train loss:0.776707098311414\n",
      "train loss:0.9040650485848635\n",
      "train loss:0.8081196333682286\n",
      "train loss:0.8293038125816833\n",
      "train loss:0.7214433115946\n",
      "train loss:0.9135756222800111\n",
      "train loss:0.8823661431607284\n",
      "train loss:0.8337820422000876\n",
      "train loss:0.954770580826004\n",
      "train loss:0.775172472996095\n",
      "train loss:0.9875847088561595\n",
      "train loss:0.9810479001015378\n",
      "train loss:0.9199740042986448\n",
      "train loss:0.9178773195386708\n",
      "train loss:0.8557997157647668\n",
      "train loss:0.8740687039671694\n",
      "train loss:0.9689121073147322\n",
      "train loss:0.7754284810600963\n",
      "train loss:0.9164739344824386\n",
      "train loss:0.8808571470906044\n",
      "train loss:0.8512345085570128\n",
      "train loss:0.8559996213142077\n",
      "train loss:0.8745304715557387\n",
      "train loss:0.782848447560822\n",
      "train loss:0.9359526962048386\n",
      "train loss:0.9041734925779268\n",
      "train loss:0.9135458676184429\n",
      "train loss:1.0828805781058128\n",
      "train loss:1.0978264508553994\n",
      "train loss:0.8611385605933651\n",
      "train loss:0.7558956597445223\n",
      "train loss:0.8074591552608652\n",
      "train loss:0.9727636033968848\n",
      "train loss:0.8043951910421256\n",
      "train loss:0.9221503859434401\n",
      "train loss:0.8784277390566294\n",
      "train loss:0.9092935235940297\n",
      "train loss:1.1931888643921165\n",
      "train loss:1.1924679615715374\n",
      "train loss:0.8652192719291314\n",
      "train loss:0.8476708196168533\n",
      "train loss:0.855725932938865\n",
      "train loss:0.9477471951507789\n",
      "train loss:0.9051691510446682\n",
      "train loss:0.806563477009805\n",
      "train loss:0.9595391799282663\n",
      "train loss:0.9357722119171248\n",
      "train loss:0.835664738977966\n",
      "train loss:1.0346210730940733\n",
      "train loss:0.8174925119134966\n",
      "train loss:1.135306365021667\n",
      "train loss:0.8224362500875151\n",
      "train loss:0.6201853458965422\n",
      "train loss:0.8052005345071522\n",
      "train loss:0.9175158293806962\n",
      "train loss:0.9488499531197863\n",
      "train loss:0.8661152402926678\n",
      "train loss:0.9819674925633056\n",
      "train loss:0.8776847499310209\n",
      "train loss:0.9417866842081248\n",
      "train loss:0.8924058703297006\n",
      "train loss:0.9570165721656185\n",
      "train loss:0.7266038261092912\n",
      "train loss:0.8322322623308734\n",
      "train loss:0.8781132907013236\n",
      "train loss:0.9182788814763775\n",
      "train loss:0.7235367578233997\n",
      "train loss:0.7957090155656371\n",
      "train loss:0.8394655308167736\n",
      "train loss:0.9576349802924291\n",
      "train loss:0.8682739930117641\n",
      "train loss:0.8339879073295734\n",
      "train loss:0.8735632232454036\n",
      "train loss:0.9551292780075914\n",
      "train loss:1.0433502837238395\n",
      "train loss:0.8373897271228664\n",
      "train loss:0.8277786252025731\n",
      "train loss:0.8647123742677972\n",
      "train loss:0.9813091688867989\n",
      "train loss:0.7785680219445773\n",
      "train loss:0.9606675066940525\n",
      "train loss:0.8396110057492318\n",
      "train loss:0.8086137605465012\n",
      "train loss:0.9401646146672838\n",
      "train loss:0.8329778750210027\n",
      "train loss:0.8931287023752845\n",
      "train loss:0.626244320907693\n",
      "train loss:0.876789450648134\n",
      "train loss:0.8626949849711869\n",
      "train loss:0.9677731840594338\n",
      "train loss:0.8714886577422816\n",
      "train loss:0.7773560809425655\n",
      "train loss:0.7884417739973573\n",
      "train loss:0.8041568349333237\n",
      "train loss:0.9336251053810842\n",
      "train loss:0.8995079125907312\n",
      "train loss:0.8878577970894491\n",
      "train loss:0.7527065349626872\n",
      "train loss:0.9113590904059973\n",
      "train loss:1.0127171891354014\n",
      "train loss:0.9283772849807619\n",
      "train loss:0.8622224948983961\n",
      "train loss:0.8914874863681596\n",
      "train loss:0.940597494596942\n",
      "train loss:0.7740097603333379\n",
      "train loss:0.8906771564074173\n",
      "train loss:0.9746276600233673\n",
      "train loss:0.9955079913459504\n",
      "train loss:0.7900057863292307\n",
      "train loss:0.6768328965994544\n",
      "train loss:0.9883171265917735\n",
      "train loss:0.9338657739550074\n",
      "train loss:1.0756673881459557\n",
      "train loss:0.9442034838566603\n",
      "train loss:0.9449821185422264\n",
      "train loss:1.081309640454733\n",
      "train loss:0.9421904901934194\n",
      "train loss:0.7927108387298243\n",
      "train loss:1.0938435017252903\n",
      "train loss:0.8671999388041607\n",
      "train loss:0.8398902203547145\n",
      "train loss:0.9689505748990851\n",
      "train loss:1.0162301113011762\n",
      "train loss:1.0203694827619556\n",
      "train loss:0.8966447577571018\n",
      "train loss:0.9609461437483887\n",
      "train loss:1.0771170981426814\n",
      "train loss:0.8790594280671208\n",
      "train loss:0.9154627983029775\n",
      "train loss:0.920378919820068\n",
      "train loss:0.9478765523299617\n",
      "train loss:0.8347249327750609\n",
      "train loss:0.9493894861753219\n",
      "train loss:0.8566834192766869\n",
      "train loss:0.7725620972682264\n",
      "train loss:0.8876614428752234\n",
      "train loss:0.7332152856791972\n",
      "train loss:0.9323106218079384\n",
      "train loss:0.8576066208767305\n",
      "train loss:0.9156080039781989\n",
      "train loss:0.8366069561602746\n",
      "train loss:1.0258825069806639\n",
      "train loss:1.0347549558424038\n",
      "train loss:0.875901198515296\n",
      "train loss:0.9032294208015585\n",
      "train loss:0.9368000063280504\n",
      "train loss:0.9456615119169623\n",
      "train loss:0.826308560782682\n",
      "train loss:0.7946363645689936\n",
      "train loss:0.8906780786209574\n",
      "train loss:0.9284465697092904\n",
      "train loss:0.8235352684884198\n",
      "train loss:0.9779641154644445\n",
      "train loss:0.863810445146171\n",
      "train loss:0.8491426260997885\n",
      "train loss:0.8963201484096863\n",
      "train loss:0.917033364423036\n",
      "train loss:0.9489435475596121\n",
      "train loss:0.9899142218261349\n",
      "train loss:0.7648487928677394\n",
      "train loss:0.9495559233215768\n",
      "train loss:0.962512826504548\n",
      "train loss:0.7313409973920431\n",
      "train loss:0.9093540570648158\n",
      "train loss:1.0125515948556774\n",
      "train loss:0.7464127993407288\n",
      "train loss:0.9386495151409102\n",
      "train loss:0.9455354947329139\n",
      "train loss:1.0344226727591168\n",
      "train loss:0.9070624193477511\n",
      "train loss:0.7777662213940153\n",
      "train loss:1.0403706781423279\n",
      "train loss:0.6634774919016142\n",
      "train loss:0.9397106429545555\n",
      "train loss:1.0066343445678096\n",
      "train loss:0.9608664654429249\n",
      "train loss:0.8989519631039657\n",
      "train loss:0.7783372744632477\n",
      "train loss:0.9129217246363436\n",
      "train loss:0.7557666983484992\n",
      "train loss:0.9376719566926621\n",
      "train loss:0.7967617182557784\n",
      "train loss:0.9343503463335968\n",
      "train loss:1.1476420556014277\n",
      "train loss:1.019156200457197\n",
      "train loss:0.7858506906058607\n",
      "train loss:0.964072719808885\n",
      "train loss:0.711158010220867\n",
      "train loss:0.8941232261991889\n",
      "train loss:0.840791951958536\n",
      "train loss:0.8743029536370471\n",
      "train loss:1.0277970673475458\n",
      "train loss:1.074479642089977\n",
      "train loss:0.7808271855367918\n",
      "train loss:0.7179561340973933\n",
      "train loss:1.0064849202021775\n",
      "train loss:1.108900746389406\n",
      "train loss:0.7743462839630988\n",
      "train loss:0.8893201511670193\n",
      "train loss:0.9192143161399958\n",
      "train loss:0.9028689669734317\n",
      "train loss:0.838629778433572\n",
      "train loss:1.0221366343199758\n",
      "train loss:1.0113678666471642\n",
      "=== epoch:7, train acc:0.991, test acc:0.981 ===\n",
      "train loss:0.8395973376622995\n",
      "train loss:0.8533325659410823\n",
      "train loss:1.04302972472102\n",
      "train loss:0.9679119304550906\n",
      "train loss:0.8518873130563356\n",
      "train loss:0.8947219341517861\n",
      "train loss:1.062674799660667\n",
      "train loss:0.8915814305346004\n",
      "train loss:0.8517049080794767\n",
      "train loss:1.1426317909270522\n",
      "train loss:0.7516402613002117\n",
      "train loss:0.9089740834976453\n",
      "train loss:0.7435384911598574\n",
      "train loss:1.0191073219461126\n",
      "train loss:0.7476577166668698\n",
      "train loss:0.8740924490629037\n",
      "train loss:0.8191997313556216\n",
      "train loss:0.8717575899090135\n",
      "train loss:0.7471377592658505\n",
      "train loss:0.8389298016542277\n",
      "train loss:0.9995444423541907\n",
      "train loss:0.9241076883129257\n",
      "train loss:0.8678387808045026\n",
      "train loss:0.9352473489397625\n",
      "train loss:0.7596105373283748\n",
      "train loss:0.66952968830459\n",
      "train loss:0.7859821154904322\n",
      "train loss:0.8193134435390487\n",
      "train loss:0.9459955609009135\n",
      "train loss:0.9884782231211782\n",
      "train loss:0.9396688770215653\n",
      "train loss:0.8326661108405397\n",
      "train loss:0.7313006895321358\n",
      "train loss:0.8339413611445786\n",
      "train loss:0.776974708337605\n",
      "train loss:0.9356723451760218\n",
      "train loss:0.8312168913511292\n",
      "train loss:0.9006790475567941\n",
      "train loss:0.8876662108026048\n",
      "train loss:0.9723843826580142\n",
      "train loss:0.8268991214620443\n",
      "train loss:0.8847480567998499\n",
      "train loss:0.7980504667995706\n",
      "train loss:0.8509464768095311\n",
      "train loss:0.8620960107102998\n",
      "train loss:0.8858987944749874\n",
      "train loss:1.0208669980177596\n",
      "train loss:0.8586284010303693\n",
      "train loss:1.0174841180297367\n",
      "train loss:1.0068543713327542\n",
      "train loss:0.8724709777925793\n",
      "train loss:0.9343958027905811\n",
      "train loss:0.946510038812957\n",
      "train loss:0.9163466911897739\n",
      "train loss:0.8493721822750662\n",
      "train loss:0.9786572377534436\n",
      "train loss:0.8058020779042987\n",
      "train loss:0.7901826278528855\n",
      "train loss:0.7978918577008585\n",
      "train loss:0.9958391473264763\n",
      "train loss:0.8180591111384957\n",
      "train loss:0.7832232542069971\n",
      "train loss:0.8477893421411136\n",
      "train loss:1.1250826509196805\n",
      "train loss:0.9012464491367456\n",
      "train loss:0.834817803389411\n",
      "train loss:0.74500928871476\n",
      "train loss:0.9377271775570466\n",
      "train loss:0.9194068577552893\n",
      "train loss:0.9771934189601883\n",
      "train loss:0.8276650426364973\n",
      "train loss:0.7833724784684906\n",
      "train loss:1.0574434723641266\n",
      "train loss:0.7587228332418543\n",
      "train loss:0.9626572421629556\n",
      "train loss:1.0242501083738709\n",
      "train loss:0.9618236177286739\n",
      "train loss:1.0517231420637285\n",
      "train loss:0.9086352638969574\n",
      "train loss:0.7778228909164145\n",
      "train loss:0.9172783557273114\n",
      "train loss:1.022932780030855\n",
      "train loss:0.8531423384189442\n",
      "train loss:0.9201986011449468\n",
      "train loss:0.9583954134372819\n",
      "train loss:1.0080130876992406\n",
      "train loss:0.829500699668754\n",
      "train loss:0.9550255811677938\n",
      "train loss:0.7526800481307774\n",
      "train loss:0.8903836961378322\n",
      "train loss:0.7331079876196539\n",
      "train loss:0.9559300376068822\n",
      "train loss:0.8162472972968041\n",
      "train loss:1.0158241171450288\n",
      "train loss:0.9260042152893281\n",
      "train loss:0.966083361023814\n",
      "train loss:0.9579377702620056\n",
      "train loss:0.7803073905749688\n",
      "train loss:0.8776466734234346\n",
      "train loss:0.9522533371842424\n",
      "train loss:0.914270081956008\n",
      "train loss:0.8466510375789076\n",
      "train loss:0.9660891094536485\n",
      "train loss:0.8323960684635268\n",
      "train loss:0.8075503424681603\n",
      "train loss:1.0763806195338566\n",
      "train loss:0.8309390324812448\n",
      "train loss:1.1080844374702599\n",
      "train loss:0.7502670430880088\n",
      "train loss:0.9890143022759683\n",
      "train loss:0.7936453502775165\n",
      "train loss:0.8503312593119908\n",
      "train loss:0.8818700307465726\n",
      "train loss:0.808672321861059\n",
      "train loss:0.8743962597501792\n",
      "train loss:0.9216746903501948\n",
      "train loss:1.060571849473192\n",
      "train loss:0.9351978616386054\n",
      "train loss:0.8619067180559274\n",
      "train loss:0.9884334680752377\n",
      "train loss:0.9055166745884641\n",
      "train loss:0.8803215836897368\n",
      "train loss:0.8932800302939681\n",
      "train loss:0.7952194815852315\n",
      "train loss:0.8397875214082879\n",
      "train loss:0.8147991179065971\n",
      "train loss:0.9807072789878021\n",
      "train loss:0.8310894306083553\n",
      "train loss:0.8660077167369912\n",
      "train loss:0.9600852245061556\n",
      "train loss:0.8440668597641657\n",
      "train loss:0.9125055410881256\n",
      "train loss:0.8888409458550852\n",
      "train loss:0.961619570639277\n",
      "train loss:1.1420496906513355\n",
      "train loss:1.0733101193923096\n",
      "train loss:0.869140669887227\n",
      "train loss:0.8603658057050477\n",
      "train loss:1.092831464188527\n",
      "train loss:0.8080847427087376\n",
      "train loss:0.9847370245973768\n",
      "train loss:0.7482307309566346\n",
      "train loss:0.8014491652087734\n",
      "train loss:0.833453514482416\n",
      "train loss:0.8340715850442709\n",
      "train loss:0.9852474472332041\n",
      "train loss:0.9658309426615879\n",
      "train loss:0.838099586614445\n",
      "train loss:0.8722878626310598\n",
      "train loss:0.8784662777082035\n",
      "train loss:0.8984737755137087\n",
      "train loss:1.0397694156499422\n",
      "train loss:0.876312197567094\n",
      "train loss:0.8449260151652765\n",
      "train loss:0.7960861776833363\n",
      "train loss:0.9017957094188813\n",
      "train loss:0.7766663223652293\n",
      "train loss:0.9334276968644769\n",
      "train loss:0.9102090859982944\n",
      "train loss:0.8911378893735181\n",
      "train loss:0.8655804097167347\n",
      "train loss:0.9266816132430978\n",
      "train loss:0.9395358004837252\n",
      "train loss:0.8267289981484933\n",
      "train loss:0.8125752033308538\n",
      "train loss:0.9128347968294436\n",
      "train loss:1.0964711928299056\n",
      "train loss:0.9328193858066527\n",
      "train loss:1.0122669530979698\n",
      "train loss:0.8013231882299111\n",
      "train loss:0.6899559218775457\n",
      "train loss:0.8735188885668134\n",
      "train loss:0.9695638900304067\n",
      "train loss:0.9744130326119538\n",
      "train loss:0.9763748775754627\n",
      "train loss:0.9259072016207648\n",
      "train loss:0.9428392017996999\n",
      "train loss:0.9549758126256133\n",
      "train loss:0.862454545091106\n",
      "train loss:0.8281778259430661\n",
      "train loss:0.811226689410983\n",
      "train loss:0.9055891664581671\n",
      "train loss:0.8793630794526329\n",
      "train loss:0.8148286754947296\n",
      "train loss:0.8633804696535919\n",
      "train loss:0.8947970126265977\n",
      "train loss:0.8177746684764203\n",
      "train loss:1.0282228427054658\n",
      "train loss:0.7922424774769026\n",
      "train loss:0.9999952706464795\n",
      "train loss:0.924381283180867\n",
      "train loss:0.9639687367786464\n",
      "train loss:0.9868931006468625\n",
      "train loss:0.8895678027968391\n",
      "train loss:0.8702630759605889\n",
      "train loss:0.8474189580173516\n",
      "train loss:0.9360995199179\n",
      "train loss:0.897550813429067\n",
      "train loss:0.9546420828421993\n",
      "train loss:0.9780707987120322\n",
      "train loss:0.9457482370871056\n",
      "train loss:0.8207398370356644\n",
      "train loss:0.7012930524066028\n",
      "train loss:0.9621675008499434\n",
      "train loss:0.8748106351782167\n",
      "train loss:0.8444067680806904\n",
      "train loss:0.7146908720930422\n",
      "train loss:0.9641569703253902\n",
      "train loss:0.848346691585661\n",
      "train loss:0.9125048238297668\n",
      "train loss:0.7999637922311029\n",
      "train loss:0.9922391722062485\n",
      "train loss:0.9042953261874637\n",
      "train loss:0.819745746748812\n",
      "train loss:0.9443248646661049\n",
      "train loss:0.9437964178285906\n",
      "train loss:0.9065197403869675\n",
      "train loss:0.8043498424218608\n",
      "train loss:0.7851143443166755\n",
      "train loss:0.9677092336653402\n",
      "train loss:0.8790115715952939\n",
      "train loss:0.8328719862507686\n",
      "train loss:0.9591530308835036\n",
      "train loss:0.8526184298365114\n",
      "train loss:0.7148863566388739\n",
      "train loss:0.9172840620872884\n",
      "train loss:0.8271289790302926\n",
      "train loss:0.9058766208502449\n",
      "train loss:0.9346553942075373\n",
      "train loss:0.893186993664294\n",
      "train loss:1.0491499163267124\n",
      "train loss:0.8484920439717386\n",
      "train loss:1.0130199584055306\n",
      "train loss:0.8761022009633862\n",
      "train loss:0.8416456256081427\n",
      "train loss:0.8362281230425579\n",
      "train loss:0.9905007915610372\n",
      "train loss:1.005154235853804\n",
      "train loss:1.0439838984988348\n",
      "train loss:0.8792126831914107\n",
      "train loss:0.9316822236827866\n",
      "train loss:0.9251261070814077\n",
      "train loss:0.8859019631009002\n",
      "train loss:0.8815550285132414\n",
      "train loss:1.2723130246300614\n",
      "train loss:0.8608894225933671\n",
      "train loss:0.8658309283260641\n",
      "train loss:0.7193664146381704\n",
      "train loss:0.9854937876979064\n",
      "train loss:0.9393264096883154\n",
      "train loss:0.9404287393150729\n",
      "train loss:1.0345341562169863\n",
      "train loss:0.9708045157632068\n",
      "train loss:0.8844701824723198\n",
      "train loss:0.9809043259339562\n",
      "train loss:0.8636723124189976\n",
      "train loss:0.9957134485608892\n",
      "train loss:0.7057712060966743\n",
      "train loss:0.9275093077721687\n",
      "train loss:0.8556455680155988\n",
      "train loss:0.8854397342773268\n",
      "train loss:0.8691026812009744\n",
      "train loss:0.8701350782009923\n",
      "train loss:0.8015923841476426\n",
      "train loss:0.8197331561281784\n",
      "train loss:1.0213262236385787\n",
      "train loss:0.9243821353074483\n",
      "train loss:0.8697501419564155\n",
      "train loss:0.8909881086511292\n",
      "train loss:0.8034804341988213\n",
      "train loss:0.8957643997891317\n",
      "train loss:0.8966766914220672\n",
      "train loss:0.9045477069159703\n",
      "train loss:1.0615721319179314\n",
      "train loss:0.9030932203690705\n",
      "train loss:0.8268973678740197\n",
      "train loss:0.796644559443871\n",
      "train loss:0.9179124419608492\n",
      "train loss:0.7088158481609098\n",
      "train loss:0.8787929981483943\n",
      "train loss:0.9168461452701058\n",
      "train loss:0.9438006827109002\n",
      "train loss:1.00802394649247\n",
      "train loss:0.8647367259055737\n",
      "train loss:1.003544007808386\n",
      "train loss:0.862812449990103\n",
      "train loss:0.7660201856444878\n",
      "train loss:1.0380790156838875\n",
      "train loss:0.8754008346016048\n",
      "train loss:0.8901741241737094\n",
      "train loss:0.8901138171334835\n",
      "train loss:0.7806885476982565\n",
      "train loss:0.8430035552544402\n",
      "train loss:0.9636250439408626\n",
      "train loss:0.9401580946469636\n",
      "train loss:0.828995901125668\n",
      "train loss:0.8379479918361674\n",
      "train loss:0.8536407424180499\n",
      "train loss:0.9080926809775547\n",
      "train loss:0.8204841000765133\n",
      "train loss:0.9709455805344246\n",
      "train loss:0.8465937597303395\n",
      "train loss:0.9787611559143814\n",
      "train loss:0.925399144280427\n",
      "train loss:0.7874443540247873\n",
      "train loss:1.1219112367252115\n",
      "train loss:0.8472301358663181\n",
      "train loss:0.8969241649728168\n",
      "train loss:1.0556396068340728\n",
      "train loss:0.8864090534718592\n",
      "train loss:1.0045250970207111\n",
      "train loss:0.771958170427563\n",
      "train loss:0.7487655199496598\n",
      "train loss:0.7890561352630574\n",
      "train loss:0.8246185549541916\n",
      "train loss:0.8466117165777741\n",
      "train loss:0.8475546455320706\n",
      "train loss:0.8131249574536641\n",
      "train loss:0.8259121477158423\n",
      "train loss:1.056290596857243\n",
      "train loss:0.8843782315887604\n",
      "train loss:0.9403698790198847\n",
      "train loss:0.8796565423218368\n",
      "train loss:0.8224730484756577\n",
      "train loss:1.014027793066444\n",
      "train loss:0.9418767733583157\n",
      "train loss:0.8732133080974391\n",
      "train loss:1.0495560754892177\n",
      "train loss:1.0645488248481112\n",
      "train loss:0.866245027726995\n",
      "train loss:0.8292948698952972\n",
      "train loss:0.9424615013767622\n",
      "train loss:0.9233194946305464\n",
      "train loss:0.9146394105226218\n",
      "train loss:0.9556021448363203\n",
      "train loss:0.9438572378483372\n",
      "train loss:0.9017401306892111\n",
      "train loss:0.9116106227842709\n",
      "train loss:0.918933047817612\n",
      "train loss:0.7986639883660991\n",
      "train loss:0.8706488263126451\n",
      "train loss:0.9026458658712836\n",
      "train loss:1.000802493658497\n",
      "train loss:0.8858298144734443\n",
      "train loss:0.9395464702773593\n",
      "train loss:0.8295370433132513\n",
      "train loss:0.7988922768577514\n",
      "train loss:0.770544233831132\n",
      "train loss:0.936496394302279\n",
      "train loss:0.8617757389355738\n",
      "train loss:1.0543296725238962\n",
      "train loss:0.9963062310544732\n",
      "train loss:0.7990637862775165\n",
      "train loss:0.7278079500396827\n",
      "train loss:0.8417925303194176\n",
      "train loss:0.7532414608566461\n",
      "train loss:0.7726060963322913\n",
      "train loss:0.8979920778356368\n",
      "train loss:0.6570266770878079\n",
      "train loss:0.7175352035606265\n",
      "train loss:0.9803624356595297\n",
      "train loss:1.0037222320392243\n",
      "train loss:0.9368853715433206\n",
      "train loss:0.810598391245913\n",
      "train loss:0.9026342726092405\n",
      "train loss:0.8053026873078227\n",
      "train loss:0.9077231637921325\n",
      "train loss:0.8491445189447134\n",
      "train loss:0.8932255364037421\n",
      "train loss:0.8874796058358537\n",
      "train loss:0.8420169179312001\n",
      "train loss:0.9767643385430358\n",
      "train loss:0.7902733154035876\n",
      "train loss:1.0329102895840259\n",
      "train loss:0.9043410515299496\n",
      "train loss:0.8609861615389187\n",
      "train loss:0.7359655175875871\n",
      "train loss:0.8156451002791963\n",
      "train loss:1.0955605678339473\n",
      "train loss:0.9541785783434753\n",
      "train loss:0.7731697237793049\n",
      "train loss:0.938660120520354\n",
      "train loss:1.1670140577670005\n",
      "train loss:0.8722374753966675\n",
      "train loss:0.8231335499890697\n",
      "train loss:0.8941419295511104\n",
      "train loss:0.8299428467227168\n",
      "train loss:0.8496813475048621\n",
      "train loss:0.9933165269585927\n",
      "train loss:1.0484925207578737\n",
      "train loss:0.8918588886764202\n",
      "train loss:0.6092580481738938\n",
      "train loss:0.9355310045276147\n",
      "train loss:0.7774969312794782\n",
      "train loss:0.7989393258513374\n",
      "train loss:0.808577523006617\n",
      "train loss:0.8321328378282742\n",
      "train loss:0.8376650302754675\n",
      "train loss:1.1698142391421902\n",
      "train loss:1.156225841753141\n",
      "train loss:0.7783433400583984\n",
      "train loss:0.9169474201988091\n",
      "train loss:0.9133735546721514\n",
      "train loss:0.7899623089030012\n",
      "train loss:0.7111125550863003\n",
      "train loss:0.9060713233923265\n",
      "train loss:0.9587537852418897\n",
      "train loss:0.8697712561829204\n",
      "train loss:0.8765831894850921\n",
      "train loss:0.8628101200402166\n",
      "train loss:0.7587171451636279\n",
      "train loss:0.9748177938858527\n",
      "train loss:0.7891940030881913\n",
      "train loss:0.9728324771129657\n",
      "train loss:0.7705508065930938\n",
      "train loss:0.9148747992467691\n",
      "train loss:0.7818412866023183\n",
      "train loss:0.8879870791947745\n",
      "train loss:0.7562334381646391\n",
      "train loss:0.7803346702374562\n",
      "train loss:0.7097616741571369\n",
      "train loss:0.860597456265773\n",
      "train loss:0.7351450900144494\n",
      "train loss:0.9333212215257821\n",
      "train loss:1.0625040807886907\n",
      "train loss:0.8362333581177822\n",
      "train loss:0.8980549013735125\n",
      "train loss:1.0682450519261795\n",
      "train loss:0.8768265505373605\n",
      "train loss:0.8490068117904821\n",
      "train loss:0.8633471004814862\n",
      "train loss:0.9553049810720025\n",
      "train loss:0.7020170613013321\n",
      "train loss:0.8388203506571533\n",
      "train loss:0.859318325640698\n",
      "train loss:0.9930501211756695\n",
      "train loss:0.8548696507296443\n",
      "train loss:0.9274886606008741\n",
      "train loss:0.7114461892281027\n",
      "train loss:0.7997107895208903\n",
      "train loss:1.0309328391223267\n",
      "train loss:1.0991733232001317\n",
      "train loss:0.9241229766346787\n",
      "train loss:0.958643968853721\n",
      "train loss:1.0039029079298885\n",
      "train loss:0.7766969994628137\n",
      "train loss:0.946975211867256\n",
      "train loss:0.8791616466229416\n",
      "train loss:0.8986869720721131\n",
      "train loss:0.7950834266531415\n",
      "train loss:0.8252213562848276\n",
      "train loss:0.7562937628114785\n",
      "train loss:0.9472373150202593\n",
      "train loss:0.7616435752205851\n",
      "train loss:0.7452357158972042\n",
      "train loss:0.7254889233180376\n",
      "train loss:1.1299322795674336\n",
      "train loss:0.8572423575815605\n",
      "train loss:1.0726453498806572\n",
      "train loss:1.022401622909719\n",
      "train loss:1.0113001361684113\n",
      "train loss:1.0996575121509164\n",
      "train loss:0.9222130959974756\n",
      "train loss:0.9538722821605564\n",
      "train loss:0.8816035840480392\n",
      "train loss:0.7086365679221034\n",
      "train loss:0.9330558654937127\n",
      "train loss:0.864855077103202\n",
      "train loss:0.9486103591150014\n",
      "train loss:0.8944892005545592\n",
      "train loss:0.9031451198844241\n",
      "train loss:0.90855179762916\n",
      "train loss:0.9273567504568899\n",
      "train loss:0.9361880451592015\n",
      "train loss:0.7905396027314427\n",
      "train loss:0.8079460341535998\n",
      "train loss:0.7981337185227138\n",
      "train loss:0.904193784584017\n",
      "train loss:0.8342222597116433\n",
      "train loss:0.9136800927114139\n",
      "train loss:0.9033652079277182\n",
      "train loss:0.8614852029558352\n",
      "train loss:0.9262411655132611\n",
      "train loss:0.9307362976943442\n",
      "train loss:0.6662523078171558\n",
      "train loss:0.8876108000974057\n",
      "train loss:0.8547215547725889\n",
      "train loss:0.8547430559806664\n",
      "train loss:1.0084970202286336\n",
      "train loss:0.9812430225339027\n",
      "train loss:0.8321831450687663\n",
      "train loss:0.9649094453107522\n",
      "train loss:0.921339211334088\n",
      "train loss:0.8473107078630265\n",
      "train loss:0.9460403824187865\n",
      "train loss:0.8324819343736327\n",
      "train loss:0.9264996375786996\n",
      "train loss:0.9030903966126133\n",
      "train loss:0.782892053157727\n",
      "train loss:0.7433887397138739\n",
      "train loss:0.9561054029971079\n",
      "train loss:0.8290055217809597\n",
      "train loss:0.9771155947434\n",
      "train loss:0.9090970572090782\n",
      "train loss:0.926696158724001\n",
      "train loss:0.83536107149597\n",
      "train loss:0.8303434318438577\n",
      "train loss:0.9825528294447039\n",
      "train loss:0.6859383306188219\n",
      "train loss:0.8354179527371848\n",
      "train loss:1.000274163598312\n",
      "train loss:0.9728129810187767\n",
      "train loss:1.0991052736535942\n",
      "train loss:0.9770851335547593\n",
      "train loss:0.8450169785713382\n",
      "train loss:1.0918453742226375\n",
      "train loss:0.7630618012483622\n",
      "train loss:0.6328985900087915\n",
      "train loss:0.8247502352220322\n",
      "train loss:0.7655479097822038\n",
      "train loss:0.8262738467598626\n",
      "train loss:0.8798054010099042\n",
      "train loss:0.9982981475909436\n",
      "train loss:0.7807363048325402\n",
      "train loss:0.8546720931686\n",
      "train loss:0.9610736728142887\n",
      "train loss:0.7871832949686838\n",
      "train loss:0.9496763718946057\n",
      "train loss:1.15318576279716\n",
      "train loss:0.8484712825923805\n",
      "train loss:1.0132397682652312\n",
      "train loss:0.8396634457338099\n",
      "train loss:0.9445900154098535\n",
      "train loss:0.8375611868334678\n",
      "train loss:0.8459533928583224\n",
      "train loss:0.9054868802439897\n",
      "train loss:0.7627157853861313\n",
      "train loss:0.9181917565309702\n",
      "train loss:0.993809316436967\n",
      "train loss:0.7126744436505441\n",
      "train loss:0.9906283289898825\n",
      "train loss:0.8304197584419056\n",
      "train loss:0.7847842796476452\n",
      "train loss:0.8645452324767571\n",
      "train loss:1.00565032719934\n",
      "train loss:0.7781275000837209\n",
      "train loss:0.8519072173473128\n",
      "train loss:0.8425048769605061\n",
      "train loss:0.8654760685765288\n",
      "train loss:0.9741336383418348\n",
      "train loss:0.891071444040707\n",
      "train loss:0.8652443847365676\n",
      "train loss:1.0347429465411617\n",
      "train loss:0.906195524653105\n",
      "train loss:0.7503106682237184\n",
      "train loss:0.7384559291087425\n",
      "train loss:0.8929120675861111\n",
      "train loss:0.7999496504836752\n",
      "train loss:0.9678449458183297\n",
      "train loss:0.9057023851404304\n",
      "train loss:0.8585303124372677\n",
      "train loss:0.9645052161777221\n",
      "train loss:0.9284719731825116\n",
      "train loss:0.8459341035771781\n",
      "train loss:1.0701245927518672\n",
      "train loss:0.9918574536325075\n",
      "train loss:0.8284189018217029\n",
      "train loss:0.8857935626398779\n",
      "train loss:0.7934987980467235\n",
      "train loss:0.7789294559290478\n",
      "train loss:0.7723214405379636\n",
      "train loss:1.0575983689806956\n",
      "train loss:1.0135585745986093\n",
      "train loss:0.9838293133520861\n",
      "train loss:0.7974390679743257\n",
      "train loss:0.6727902305745073\n",
      "train loss:0.7699382157274871\n",
      "train loss:0.81734920842217\n",
      "train loss:0.8716296302628546\n",
      "train loss:1.0123316426871176\n",
      "train loss:0.8615423937380673\n",
      "train loss:0.9379971554673806\n",
      "train loss:0.9457675505123037\n",
      "train loss:0.9543714806237343\n",
      "train loss:0.8400733626467182\n",
      "train loss:0.8911240792614258\n",
      "train loss:0.8725664373458574\n",
      "train loss:0.8671804900598112\n",
      "train loss:0.9962078383708035\n",
      "train loss:0.8448303462772081\n",
      "train loss:0.8393947436729161\n",
      "train loss:0.7765078740180442\n",
      "train loss:0.9766218323666969\n",
      "train loss:0.8407216137528793\n",
      "train loss:0.95316647958563\n",
      "train loss:0.8705168488488273\n",
      "train loss:0.8054517562718518\n",
      "train loss:1.041808481407178\n",
      "train loss:0.9516404088150651\n",
      "train loss:0.8617066913925392\n",
      "=== epoch:8, train acc:0.995, test acc:0.987 ===\n",
      "train loss:0.9926178269621835\n",
      "train loss:0.8538326955597176\n",
      "train loss:0.9070084411961195\n",
      "train loss:0.7470171249060555\n",
      "train loss:0.8090932978481937\n",
      "train loss:0.8996334522110249\n",
      "train loss:0.9111808934723156\n",
      "train loss:0.8381495030603108\n",
      "train loss:0.8760531319442468\n",
      "train loss:0.8828074221404693\n",
      "train loss:0.9536285931062928\n",
      "train loss:1.0537229783577715\n",
      "train loss:0.8739122369290736\n",
      "train loss:0.986254666587483\n",
      "train loss:0.8117983453713162\n",
      "train loss:0.8683421692198746\n",
      "train loss:0.8697304369996278\n",
      "train loss:0.8276588347647857\n",
      "train loss:0.9439366259866422\n",
      "train loss:0.8646002058950457\n",
      "train loss:0.7935170258363624\n",
      "train loss:1.1624036611928947\n",
      "train loss:0.7989138500830459\n",
      "train loss:0.8754131325956673\n",
      "train loss:0.8853427885566285\n",
      "train loss:1.0305286816741144\n",
      "train loss:0.9888841643126278\n",
      "train loss:0.8541203329405532\n",
      "train loss:0.6629881127359256\n",
      "train loss:0.7909727361398302\n",
      "train loss:0.8113980393672356\n",
      "train loss:0.875487289572949\n",
      "train loss:0.9880428517736681\n",
      "train loss:0.9981723924405064\n",
      "train loss:0.8829776224948519\n",
      "train loss:0.8934366494231607\n",
      "train loss:0.9161259245197795\n",
      "train loss:0.9372226596885916\n",
      "train loss:0.9934601119203563\n",
      "train loss:0.7569000614251139\n",
      "train loss:0.8111972531939471\n",
      "train loss:0.6873697931262166\n",
      "train loss:0.7828242695575477\n",
      "train loss:0.9546224838292214\n",
      "train loss:1.039043664957712\n",
      "train loss:1.002467900073201\n",
      "train loss:1.0750921532853788\n",
      "train loss:0.7969008552673236\n",
      "train loss:0.8056861219558994\n",
      "train loss:0.891082184549425\n",
      "train loss:0.9648843235487365\n",
      "train loss:0.8409322954538853\n",
      "train loss:0.8647240158591001\n",
      "train loss:0.8570986008587872\n",
      "train loss:0.9247947859650621\n",
      "train loss:0.7585551206286617\n",
      "train loss:0.9065370331228898\n",
      "train loss:0.9576191690170666\n",
      "train loss:0.7475566736587322\n",
      "train loss:0.798013820846589\n",
      "train loss:0.8817673805599908\n",
      "train loss:0.8005099762022176\n",
      "train loss:0.9246522452996048\n",
      "train loss:0.8244036953380137\n",
      "train loss:0.6934403631595754\n",
      "train loss:0.9994360296476912\n",
      "train loss:0.8496717349940179\n",
      "train loss:0.8123448466624161\n",
      "train loss:1.230971794823742\n",
      "train loss:0.9604449569365291\n",
      "train loss:0.9037277150326082\n",
      "train loss:0.8674828102669936\n",
      "train loss:0.8951673729071637\n",
      "train loss:0.9229983063993598\n",
      "train loss:1.0162039091916553\n",
      "train loss:1.026097963752652\n",
      "train loss:0.8928897766625616\n",
      "train loss:0.8010588865867924\n",
      "train loss:0.9410948255126699\n",
      "train loss:0.887563186782821\n",
      "train loss:0.7886451950342657\n",
      "train loss:0.9096266038778364\n",
      "train loss:0.9532563093904624\n",
      "train loss:0.7604045429796846\n",
      "train loss:0.9567013771351098\n",
      "train loss:0.959469254377322\n",
      "train loss:0.9178485535703474\n",
      "train loss:0.9556380578903131\n",
      "train loss:0.860105368878434\n",
      "train loss:0.9060530434735178\n",
      "train loss:0.9684362917839916\n",
      "train loss:0.904003457533586\n",
      "train loss:0.990983100827561\n",
      "train loss:0.9746755912732976\n",
      "train loss:0.7990927724305675\n",
      "train loss:0.671990547638806\n",
      "train loss:0.9345842719160484\n",
      "train loss:0.7448656195192064\n",
      "train loss:0.7603941984080782\n",
      "train loss:0.8294899710149202\n",
      "train loss:0.8761453011523501\n",
      "train loss:0.9108298857205993\n",
      "train loss:0.8031451410275513\n",
      "train loss:1.0017435434457125\n",
      "train loss:0.8264255287453472\n",
      "train loss:0.9560348537301927\n",
      "train loss:0.7421856605817736\n",
      "train loss:0.7965744036603732\n",
      "train loss:0.8740320301488927\n",
      "train loss:0.6818477883952602\n",
      "train loss:0.626337773830566\n",
      "train loss:0.9367016921666296\n",
      "train loss:0.8514376397204546\n",
      "train loss:0.7732776558178357\n",
      "train loss:0.9015756709204977\n",
      "train loss:0.9441864281793727\n",
      "train loss:0.8998954456266182\n",
      "train loss:0.8141528437009444\n",
      "train loss:0.5116269662060181\n",
      "train loss:0.8323556695660105\n",
      "train loss:1.0831176071397635\n",
      "train loss:0.9272254172411258\n",
      "train loss:0.9258551959456621\n",
      "train loss:0.8080986346156652\n",
      "train loss:0.8691458988443154\n",
      "train loss:0.7483821019905512\n",
      "train loss:0.8427290644030551\n",
      "train loss:0.9727641584377388\n",
      "train loss:0.8621941422604152\n",
      "train loss:0.8094953459920728\n",
      "train loss:0.9005928545932929\n",
      "train loss:0.9501141741862935\n",
      "train loss:0.9545155047525528\n",
      "train loss:0.7885584146440826\n",
      "train loss:0.9048038798603467\n",
      "train loss:0.7725821717193605\n",
      "train loss:0.9544037709048798\n",
      "train loss:0.9428371784535559\n",
      "train loss:0.9080548830210639\n",
      "train loss:0.7693779979017201\n",
      "train loss:0.8646576660455476\n",
      "train loss:1.0323976193155615\n",
      "train loss:0.8923977000869621\n",
      "train loss:0.7596935510247408\n",
      "train loss:0.956035575075481\n",
      "train loss:0.8759251009254989\n",
      "train loss:0.7282464997717985\n",
      "train loss:1.0934606180499384\n",
      "train loss:0.7575401816578011\n",
      "train loss:0.8098119750631666\n",
      "train loss:0.8253303228927732\n",
      "train loss:0.7355042829773153\n",
      "train loss:0.7310844668160208\n",
      "train loss:0.760258654161749\n",
      "train loss:0.8816630366093888\n",
      "train loss:1.0018090024170008\n",
      "train loss:0.7517107971577617\n",
      "train loss:0.978851494637736\n",
      "train loss:0.835179862366871\n",
      "train loss:0.965076619987467\n",
      "train loss:0.7396982631698873\n",
      "train loss:0.7211221587866317\n",
      "train loss:0.7985878952317664\n",
      "train loss:0.7507658925014455\n",
      "train loss:0.9019276769524387\n",
      "train loss:0.8946808151016797\n",
      "train loss:0.790592272820022\n",
      "train loss:0.7818727206702097\n",
      "train loss:0.7927383045318431\n",
      "train loss:0.9223815097532628\n",
      "train loss:1.0119742254722432\n",
      "train loss:0.8800463573434718\n",
      "train loss:0.8114006359172992\n",
      "train loss:0.9448580432224486\n",
      "train loss:0.8417056452493011\n",
      "train loss:0.8438406042093934\n",
      "train loss:0.9443932387370887\n",
      "train loss:1.116653313316521\n",
      "train loss:0.9067070611481263\n",
      "train loss:0.9565975156955123\n",
      "train loss:0.9192194542130783\n",
      "train loss:0.9897706166168372\n",
      "train loss:0.8606861069221471\n",
      "train loss:0.8909111660752034\n",
      "train loss:0.9956734027086991\n",
      "train loss:0.7132367804252613\n",
      "train loss:0.8507461049710789\n",
      "train loss:0.8073073617778257\n",
      "train loss:0.8237590353348954\n",
      "train loss:0.9703319211219532\n",
      "train loss:0.9265341489814412\n",
      "train loss:1.0262233350264973\n",
      "train loss:0.9243676301210221\n",
      "train loss:0.8889988810968467\n",
      "train loss:0.9056447044205623\n",
      "train loss:0.8721652492280972\n",
      "train loss:0.9494794799175428\n",
      "train loss:0.7451327665546611\n",
      "train loss:0.8523289362312312\n",
      "train loss:0.7171526014830789\n",
      "train loss:1.0377471208186544\n",
      "train loss:0.7764595870830207\n",
      "train loss:0.9573363066223122\n",
      "train loss:0.9413827660831108\n",
      "train loss:0.6948469338059587\n",
      "train loss:0.8786271155638622\n",
      "train loss:0.8736315788891824\n",
      "train loss:0.8233384737155527\n",
      "train loss:0.9170259268073778\n",
      "train loss:0.7372134532483419\n",
      "train loss:0.9266549683595617\n",
      "train loss:0.9320324549676018\n",
      "train loss:0.8169785067111736\n",
      "train loss:1.0013104710073555\n",
      "train loss:0.9208064855855421\n",
      "train loss:0.9370591304937108\n",
      "train loss:0.908262640784131\n",
      "train loss:0.8187279652569175\n",
      "train loss:0.8847125889984487\n",
      "train loss:0.9657242379440325\n",
      "train loss:0.7745295320053032\n",
      "train loss:0.9890209517197319\n",
      "train loss:0.7735098864385981\n",
      "train loss:0.8600165328307642\n",
      "train loss:0.8564997626699058\n",
      "train loss:0.6679286942607368\n",
      "train loss:0.8448709369550521\n",
      "train loss:0.7665296141053841\n",
      "train loss:0.6930969222173498\n",
      "train loss:1.0380943542507162\n",
      "train loss:0.8478317656943389\n",
      "train loss:0.8096695823046131\n",
      "train loss:0.9581210795004425\n",
      "train loss:0.9518020665685666\n",
      "train loss:0.7134994316595399\n",
      "train loss:0.777076336826379\n",
      "train loss:0.9845119520896266\n",
      "train loss:0.8934021290574613\n",
      "train loss:0.7554510292000601\n",
      "train loss:0.8923411250893338\n",
      "train loss:0.83846298909051\n",
      "train loss:0.947344339616774\n",
      "train loss:0.8648088800734393\n",
      "train loss:0.8406486271316013\n",
      "train loss:1.1113246178716336\n",
      "train loss:0.9634971634391348\n",
      "train loss:0.985055650599786\n",
      "train loss:0.8425582302712755\n",
      "train loss:0.7180278964844286\n",
      "train loss:0.8913289169973757\n",
      "train loss:0.8023864497229032\n",
      "train loss:0.9048325032812897\n",
      "train loss:0.8455927845847124\n",
      "train loss:0.9221913690721405\n",
      "train loss:0.9822419168168273\n",
      "train loss:1.1882421943629267\n",
      "train loss:0.8146848785965353\n",
      "train loss:0.735213186827508\n",
      "train loss:0.9225853673132436\n",
      "train loss:0.9919243299187623\n",
      "train loss:0.8633487503181162\n",
      "train loss:0.7242594080811748\n",
      "train loss:0.9907625642750253\n",
      "train loss:0.8803375585740141\n",
      "train loss:0.8709879759546436\n",
      "train loss:1.018913085383928\n",
      "train loss:0.8880410475494526\n",
      "train loss:0.8672006874629369\n",
      "train loss:0.9090925334202715\n",
      "train loss:0.8614931321425466\n",
      "train loss:0.8117513131944927\n",
      "train loss:1.0118383378059215\n",
      "train loss:1.0450217964382815\n",
      "train loss:0.8213816653984212\n",
      "train loss:0.874925403383051\n",
      "train loss:0.9540529826226686\n",
      "train loss:0.9182184308683832\n",
      "train loss:0.9595251909932259\n",
      "train loss:0.9699156502526588\n",
      "train loss:0.9023911587852733\n",
      "train loss:0.8659180242704427\n",
      "train loss:0.8266206374720659\n",
      "train loss:0.9610966296076748\n",
      "train loss:0.7583791846537369\n",
      "train loss:0.7440004856913421\n",
      "train loss:0.9669746400488095\n",
      "train loss:0.7261075167508909\n",
      "train loss:0.8088422494212589\n",
      "train loss:1.0090163561692076\n",
      "train loss:1.0068924138923045\n",
      "train loss:0.7412509804248367\n",
      "train loss:0.8616351183421961\n",
      "train loss:0.9203318159510282\n",
      "train loss:0.8278893083769034\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\TIANAI~1\\AppData\\Local\\Temp/ipykernel_44500/2592473543.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                   evaluate_sample_num_per_epoch=1000)\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# 保存参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Desktop\\code\\ch08\\..\\common\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Desktop\\code\\ch08\\..\\common\\trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mt_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Desktop\\code\\ch08\\deep_convnet.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;31m# forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Desktop\\code\\ch08\\deep_convnet.py\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_flg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Desktop\\code\\ch08\\deep_convnet.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_flg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Desktop\\code\\ch08\\..\\common\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mout_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mFW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim2col\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m         \u001b[0mcol_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Desktop\\code\\ch08\\..\\common\\util.py\u001b[0m in \u001b[0;36mim2col\u001b[1;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mcol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_max\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx_max\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mout_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mout_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ch08/train_deepnet.py\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from deep_convnet import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 保存参数\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.4 运算精度的位数缩减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caluculate accuracy (float64) ... \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\TIANAI~1\\AppData\\Local\\Temp/ipykernel_44500/2484233243.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"caluculate accuracy (float64) ... \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# 转换为float16型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Desktop\\code\\ch08\\deep_convnet.py\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(self, x, t, batch_size)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mtt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_flg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0macc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Desktop\\code\\ch08\\deep_convnet.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_flg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Desktop\\code\\ch08\\..\\common\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0marg_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m     \"\"\"\n\u001b[1;32m-> 1193\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# half_float_network.py\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_convnet import DeepConvNet\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "\n",
    "sampled = 10000 # 为了实现高速化\n",
    "x_test = x_test[:sampled]\n",
    "t_test = t_test[:sampled]\n",
    "\n",
    "print(\"caluculate accuracy (float64) ... \")\n",
    "print(network.accuracy(x_test, t_test))\n",
    "\n",
    "# 转换为float16型\n",
    "x_test = x_test.astype(np.float16)\n",
    "for param in network.params.values():\n",
    "    param[...] = param.astype(np.float16)\n",
    "\n",
    "print(\"caluculate accuracy (float16) ... \")\n",
    "print(network.accuracy(x_test, t_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7bebbcdb67a87c960e72e49b9762de1591139e5dbbdfaa6c9d42133d04bebfbc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
